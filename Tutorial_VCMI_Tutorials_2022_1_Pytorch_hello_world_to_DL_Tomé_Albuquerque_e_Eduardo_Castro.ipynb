{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial_VCMI_Tutorials_2022_1_Pytorch_hello_world_to_DL_Tomé_Albuquerque_e_Eduardo_Castro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee1c86c531eb4dd89f7afd4cb7b7beec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c50a1fcf1fd94091876df8a0383d2cf4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aa6360310a314786957014de31e72cfb",
              "IPY_MODEL_b2317f5a7be341daa29ceb8900c5a956",
              "IPY_MODEL_6db541361bff482995e619d0e32e49b8"
            ]
          }
        },
        "c50a1fcf1fd94091876df8a0383d2cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa6360310a314786957014de31e72cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eca6bcdab5ba4307be1245b3134179da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b219d60e10924c17ad1572b12b3a867e"
          }
        },
        "b2317f5a7be341daa29ceb8900c5a956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef7361a3564843059647697c1d779d40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8138263adf7344279c40a19c8bd3ed38"
          }
        },
        "6db541361bff482995e619d0e32e49b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31c29ab3b6d04ade977188acce17bf9f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 19.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_317f6cc092434b9485982c87ab203642"
          }
        },
        "eca6bcdab5ba4307be1245b3134179da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b219d60e10924c17ad1572b12b3a867e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef7361a3564843059647697c1d779d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8138263adf7344279c40a19c8bd3ed38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31c29ab3b6d04ade977188acce17bf9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "317f6cc092434b9485982c87ab203642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManJ-PC/HelloCodeSchoolProject/blob/master/Tutorial_VCMI_Tutorials_2022_1_Pytorch_hello_world_to_DL_Tom%C3%A9_Albuquerque_e_Eduardo_Castro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SPnWRPay8ky"
      },
      "source": [
        "\n",
        "# Outline\n",
        "---\n",
        "\n",
        "### Motivation\n",
        "\n",
        "### Basics\n",
        "* From numpy to pytorch;\n",
        "* Autograd;\n",
        "* CUDA tensors;\n",
        "* The non-existent fit method;\n",
        "* **Example 1** - CIFAR 10 Classification;\n",
        "\n",
        "### Intermediate topics\n",
        "\n",
        "* Custom Datasets;\n",
        "* Data Augmentation;\n",
        "* Pretrained Models;\n",
        "* Custom Layers/Models;\n",
        "* **Example 2** - Multiclass Classification model for Cervical Cancer risk assessment;\n",
        "\n",
        "### Closing remarks\n",
        "* Some tips!\n",
        "* Next Tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx4ntX2GMRnF"
      },
      "source": [
        "#Motivation\n",
        "\n",
        "---\n",
        "\n",
        "### Deep Learning -  a loose definition\n",
        "\n",
        "Deep learning is the use models with **many layers**, optimized using **gradient descent** where gradients are computed by **backpropagation**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Many Layered Model:\n",
        "<center> $ \\displaystyle \\hat{y} = \\sigma(V^T\\sigma(W^Tx))$ </center>\n",
        "\n",
        "#### Gradient Descent:\n",
        "<center> $ \\displaystyle W_{t+1} = W_{t} - \\gamma \\frac{\\partial}{\\partial W} C \\qquad \\qquad V_{t+1} = V_{t} - \\gamma \\frac{\\partial}{\\partial V} C $ </center>\n",
        "\n",
        "#### Backpropagation:\n",
        "<center> $ \\displaystyle \\frac{\\partial}{\\partial V} C = \n",
        "\\frac{\\partial \\hat{y}}{\\partial V}\n",
        "\\frac{\\partial}{\\partial \\hat{y}} C \\qquad \\qquad\n",
        "\\frac{\\partial}{\\partial W} C = \n",
        "\\frac{\\partial z_1}{\\partial W}\n",
        "\\frac{\\partial \\hat{y}}{\\partial z_1}\n",
        "\\frac{\\partial}{\\partial \\hat{y}} C$ </center>\n",
        "\n",
        "\\\\\n",
        "<center> $ \\displaystyle  $ </center>\n",
        "\n",
        "<center>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Hh-HJwXQdAm2mESorc5V8dFi7I_Hb7b2\" width=\"350\" hspace=\"30\">\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Fxm0yU5xiRNZwVlElJm5q0WT9XLLx-bZ\" width=\"350\" hspace=\"30\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1e9N27Opgm8aocKJ3e2MuUyjVA-e73sX6\" width=\"280\" hspace=\"30\">\n",
        "\n",
        "Figure 1. Bottom-left image was reproduced from [[1](https://medium.com/hackernoon/gradient-descent-aynk-7cbe95a778da)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bif8bv3obyvZ"
      },
      "source": [
        "**We can do all that in NumPy. Why use PyTorch?**\n",
        "\n",
        "* Some functions/classes are already implemented (torch.nn);\n",
        "* Easy to use the GPU (accelaration);\n",
        "* Automatic differentiation;\n",
        "\n",
        "**We can do all that in Keras. Why use PyTorch?**\n",
        "* more control over what is happening since Keras is a \"high-level\" package;"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basics\n",
        "---"
      ],
      "metadata": {
        "id": "Pw1RBVM4pCoZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3ODnUK8_aKt"
      },
      "source": [
        "## From numpy to pytorch\n",
        "\n",
        "Manipulating variables in numpy and pytorch is similar:\n",
        "\n",
        "Numpy | Pytorch\n",
        "--- | ---\n",
        "np.array ( [ 1, 2, 3, 4, 5, 6 ] ) | torch.tensor([ 1, 2, 3, 4, 5, 6 ])\n",
        "np.arange (0, 10) | torch.arange(0, 10)\n",
        "array.shape | tensor.shape\n",
        "array1 + array2 | tensor1 + tensor2\n",
        "np.concatenate | torch.cat\n",
        "np.max | torch.max\n",
        "np.exp | torch.exp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaLNrREUA7J5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23fd2269-78bc-4a80-efc0-2fab35011ff2"
      },
      "source": [
        "# the same code in numpy and pytorch\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# create an array\n",
        "array = np.array([1,2,3,4,5,6], dtype=float)\n",
        "array = array.reshape((2, 3))\n",
        "print(\"1. Array Creation:\")\n",
        "print(array, \"\\nshape:\", array.shape)\n",
        "\n",
        "print(\"\\n2. Element-wise multiplication and concatenation:\")\n",
        "# element-wise multiplication\n",
        "array = array * 2\n",
        "array = np.concatenate((array, array))\n",
        "print(array)\n",
        "\n",
        "print(\"\\n3. Maximum of array:\")\n",
        "value = np.max(array)\n",
        "print(value)\n",
        "\n",
        "print(\"\\n4. Tensor Creation:\")\n",
        "# create a tensor (try to add requires_grad=True to torch.tensor and run again)\n",
        "tensor = torch.tensor([1,2,3,4,5,6], dtype=float)\n",
        "tensor = tensor.reshape((2, 3))\n",
        "print(tensor, \"\\nshape:\", tensor.shape)\n",
        "\n",
        "print(\"\\n5. Element-wise multiplication and concatenation:\")\n",
        "# element-wise multiplication\n",
        "tensor = tensor * 2\n",
        "tensor = torch.cat((tensor, tensor))\n",
        "print(tensor)\n",
        "\n",
        "print(\"\\n6. Maximum of tensor:\")\n",
        "value = torch.max(tensor)\n",
        "print(value)\n",
        "\n",
        "print(\"\\n7. Back to numpy:\")\n",
        "print(tensor.detach().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Array Creation:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]] \n",
            "shape: (2, 3)\n",
            "\n",
            "2. Element-wise multiplication and concatenation:\n",
            "[[ 2.  4.  6.]\n",
            " [ 8. 10. 12.]\n",
            " [ 2.  4.  6.]\n",
            " [ 8. 10. 12.]]\n",
            "\n",
            "3. Maximum of array:\n",
            "12.0\n",
            "\n",
            "4. Tensor Creation:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], dtype=torch.float64) \n",
            "shape: torch.Size([2, 3])\n",
            "\n",
            "5. Element-wise multiplication and concatenation:\n",
            "tensor([[ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.],\n",
            "        [ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.]], dtype=torch.float64)\n",
            "\n",
            "6. Maximum of tensor:\n",
            "tensor(12., dtype=torch.float64)\n",
            "\n",
            "7. Back to numpy:\n",
            "[[ 2.  4.  6.]\n",
            " [ 8. 10. 12.]\n",
            " [ 2.  4.  6.]\n",
            " [ 8. 10. 12.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practise\n",
        "\n",
        "Consider the following regression model:\n",
        "\n",
        "$$\\hat{y}=w^Tx, \\qquad x, w\\in \\mathbb{R}^d$$\n",
        "\n",
        "\n",
        "By the linear least squares method we know that the following solution minimizes the sum of the squarred errors for all points:\n",
        "\n",
        "$$w = (X^TX)^{-1}X^Ty$$\n",
        "\n",
        "Can you implement that in pytorch?\n",
        "\n"
      ],
      "metadata": {
        "id": "OtKQiGS_oBtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# generate toy data\n",
        "n = 100\n",
        "d = 1\n",
        "trans = torch.randn(d)\n",
        "error = torch.randn(n) * 0.1\n",
        "X = torch.randn(n, d)\n",
        "y = torch.matmul(X, trans) + error\n",
        "\n",
        "plt.scatter(X.numpy(), y.numpy())\n",
        "\n",
        "# your solution goes here\n",
        "XX_inv = torch.inverse(torch.matmul(X.T, X))\n",
        "Xy = torch.matmul(X.T, y)\n",
        "w = torch.matmul(XX_inv, Xy)\n",
        "\n",
        "print(w)\n",
        "axis_x = torch.linspace(-3, 3, 50)\n",
        "plt.plot(axis_x, w * axis_x, c=\"r\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "878yiNqVqnNw",
        "outputId": "759b18ff-5722-43a8-8a79-b2b77e2c8436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.8226])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe5c3a1c6d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3zO9f/H8cd7szTEiPpmEVEjx5UkFFLmEIZKRSX7UiGHtFjl21FWI+fKL0NJojC+oSHklJBDI3Qg1fSVQyuymO39++PaZodrm9m169q1Pe+3m9uN6/C5XteNnnv3/rze77ex1iIiIt7Lx9MFiIhIwSjIRUS8nIJcRMTLKchFRLycglxExMuV8sSHVq5c2daoUcMTHy0i4rW+/vrrY9baKlkf90iQ16hRg23btnnio0VEvJYx5pCzxzW1IiLi5RTkIiJeTkEuIuLlFOQiIl5OQS4i4uU80rVyMWJ2xBMVu5/DCYlUDfAnPCSI0OBAT5clIuJxXhHkMTviiVgYR2JSMgDxCYlELIwDUJiLSInnFVMrUbH700M8TWJSMlGx+88/MH8+fPGFmysTEfE8rwjywwmJuT+ekgKRkdC6NQweDH//7b7iREQ8zCuCvGqAf+6P+/jA+vWOEJ88GRo1cvxZRKQE8IogDw8Jwt/PN9Nj/n6+hIcEnX+gbFmYOBHWrgVroVUrGDYMTp92b7EiIm7mFUEeGhzImO4NCAzwxwCBAf6M6d7A+Y3OVq1g1y4YMAAmTIDGjWHjRrfXLCLiLsYTZ3Y2adLEumXTrDVroG9fOHTIMTp/9VXwdz5NIyJS1BljvrbWNsn6uFeMyC9amzbwzTfw+OPw5puO0fmmTZ6uSkTEpYp3kANcdhm89RasWgX//AMtW0J4OCQ674QREfE2xT/I07RtC3Fx0K8fjB0LwcGwebOnqxIRKbCSE+QA5cvDtGmwYoWjm6VFCxgxwjFSFxHxUiUryNPcdZdjdN63L7zxBtx4I2zZ4umqREQuSskMcoAKFeDdd+Gzz+DkSbj1VoiIgDNnPF2ZiEi+lNwgTxMSArt3Q58+jmX+N90EOk9URLyIghwco/PoaFi2DBISoFkzeP55jc5FxCsoyDPq0MExOn/oIRg9Gpo0ge3bPV2ViEiuFORZBQTAzJnw6adw/Dg0bQovvABnz2Z6WcyOeFpErqbmyKW0iFxNzI54DxUsIiWdgjwnnTrBnj3Qqxe8/LIj0HfuBM4fdBGfkIjl/EEXCnMR8YQCB7kxppoxZo0x5ltjzB5jzBBXFFYkVKwI770HS5bAkSNw883w0ku8uWxP3gddiIi4iStG5OeA4dbaG4BmwEBjzA0uuG7R0bmzY3R+//3w4ou8PekJ6v5+INvLcjoAQ0SkMBU4yK21v1lrt6f+/iSwFyh+B2lWqgSzZ0NMDP86/QeL33uKJzfOpVTyufSX5HQAhohIYXLpHLkxpgYQDHzl5Ln+xphtxphtR48edeXHulfXrmxeso4VdVsyfMMcFs0eTtDRn7IfdCEi4iYuC3JjTDlgATDUWvtX1uettf9nrW1irW1SpUoVV32sR3RqU59zsz9geM9RXHXyGP+dNZSBmz7CnDuX95tFRFzMJUFujPHDEeJzrLULXXFNb7DsuubcFfYWsdffyqDV73Ft17v4/JPV6c+rRVFE3MEVXSsGiAb2WmvfLHhJ3iEqdj+JScn8UaYCT3YdwYCuI6macITb7m8PkZEs3npILYoi4hauGJG3AB4C7jDG7Ez91dEF1y3SsnaoLKvTknZhb/F5rZshIoLa3dsT+NvBTK9Ri6KIFIbifWZnIWoRuZp4J+2GgRUuZeO1xzjxaH/Knk3kzdt68e7N3Ujx8T3/mgB/DickUjXAn/CQIOeHSIuIZFEyz+wsROEhQfj7+WZ6zN/Pl/D2daBnTx4eFs2aWk2IWDuLT+Y8Q63jvwBgQNMtIuJSCvKLFBocyJjuDQgM8MfgGGWP6d4gfXT973tuZdi9oxjcOZyaJw6zbOZg+m1ZiEnRilARca1Sni7Am4UGB+Y4LZL2eFS50rSr3pDXVkzluTUzCPnuS8I7DuVgpfPv04pQESkIjcgLUWhwIOEhQZyqWJl+3Z5j6N3DqX38F5bPfJKwrTH4pI7OtSJURApCQV7I0toUMYaYem24K+wt1tdozKjV05n3YQR1Tv5PK0JFpEAU5IUs67TJ0XKV6Nd9FE91Gsb1xw6xZPogQtd9AikpHqpQRLydgryQOZ02MYaF9dtyV9hbbKreEIYOhdat4Ycf3F6fiHg/BXkhc9ammOb3yy6nT/dRjL4nHL75Bho1gsmTNToXkXxRkBeyjG2KThnD9FqtHPudt2oFgwfDHXfAgez7nYuIOKMgd4PQ4EA2jrwjxzCvGuAPgYGwdClER8OOHdCwIUydqtG5iORJQe5GOa4GTetaMQb69oXdu6FlSxg0CO68Ew4edHI1EREHLQhyo/RFQrH7c99rpVo1WL7cMTp/6ilo0ACioohpejdRK7/XPi0ikok2zSrqfv4ZwsJg1Sq+rNGYp9sPJr7CFYBjNJ9xWwARKd60aZa3ql4dVqzg9W7DaHB4P7EzBvLAzs/A2kz7tOgQC5GSS0HuDYzhnevb0r7vFHZddR1jYqfw/vz/UPWv3zmckEjMjngdYiFSginIvUTVAH9+rXAlvXu+yvPtBnBT/F5iowfS//s1RH22z7ENQAbaVVGk5FCQe4m0jhdrfPgguCMhfaew51+1iVg4jtHvjuBffx3L9p6M2wNo6kWk+NLNTi8SsyM+veOlgr8fp8+c5f6tnzLyi1mcM7680rYfHze409HGCPgaQ4q1VPD34++z50hKPv93rRulIt4np5udCnIvlfGouep//EbU8onc8stuVl/bhIj2gzhyWeU8rxEY4M/GkXcUdqki4iLqWilmMk6b/FzxKu5/4DVebNufW3+OY2X0QHrEfQ55/JBOu1GqKRcR76Yg91JZd1W0xodZTbrQvu9k9lW5hnHLxjN9wctccfJ4jteo4O+nbheRYkBB7qXCQ4IwTh4/VLEq9z8whpfv6EfLQ7tYGT2AbrtXZxud+/v5YgzqdhEpBhTkXio0OJBezao7DfMUH19m3NyVDo9O5vvK1Rm/9E2mL3qV2sknMx0UnXA6yem1dYaoiHdRkHuxV0MbML5nYwID/DE4ulQyOlgpkPsejOS1NmG0ObSTVdEDONgggY0j2hAaHJjjWaE6Q1TEuyjIvVzaFrkHIzsx7r5G2XZXLF36Em4Y9xK+u3ZCUBD07g3du8ORI3nvxigiXkFBXoxkPMQi4xRKaHAg1KkDGzbAG284dlasV4/Q/esZ062+89eLiNdQH3lJtHcv9OkDW7Y4Rudvvw1XXOHpqkQkD+ojl/Pq1oWNGyEyEj79FOrVg/nzAS3lF/FGCvKSqlQpGDHCcaxczZrQsyfx7Trzxuz16isX8TIK8pLuhhtg0yZ47TWqrI5lybTH6bBvQ/rT6isXKfoU5OIYnUdE0PmR8RwuX4W3F0cyefHrVDz9J6C+cpGiziVBboyZYYz53Riz2xXXE884dV1duvceS9RtDxHy3ZesiB5IyHeb1FcuUsS5akQ+C2jvomuJh4SHBOF3aWmmNu9Jl0fGc+Syy5m26DXeWjYWjue8Z4uIeJZLgtxauw444YprieeEBgfS46ZADLDvipqEPjSON1v24oaNK/gnqC4sXuzpEkXEiVLu+iBjTH+gP0D16tXd9bGST2v2HSVtZcE531JMavEAq2rfwvjlEwgKDWVRvTZM6z6Yx7vdDJB+0EXVAH/CQ4K0mEjEA1y2IMgYUwP41FpbP6/XakFQ0VVz5FKc/YvwS05i4JfzGfjlfE6UqcCzIQP5vPYtmV/jY/DzNZxOSgGgYhk/XuhcD1Dgi7hCoZ8QpCAvHjKePORMvSM/MnbpeOoe/YkF9e/gpbb9+evScjm+3tfH4AMkpeiYOZGC0spOuSDONtLKaM+VtejyyHgmNr+frnvWsiJ6AK1/3Jrj65NTbKYQB/Wmi7iaq9oP5wJfAkHGmF+NMWGuuK64n7ONtyqW8cv0miRfP8bf1pvQh9/kz0vLMeuTl3hj2QTK/3Pqgj9HvekirqNNsyRPMTviiVgYl+00IYBLziUxeNNcntj8Cb+XrcjIDoP54tqb8rymDn4Wyb9CnyPPDwW594nZEU9U7H7iExIxkO2GaMPfvmPs0glcf/xnPmrYjtF3hHGydFmn10qbIwfdBBXJDwW5uEzMjnhe+u8e/shyVFzpc2cZuuFD+m9ZyP/KXc6IDoPZUDM4/XkD6YENZBvl6yaoSO50s1NcJjQ4kDKXZF+CcKbUJbzeug89ekeR6FeaD+aP4rXPplDuzGkqlvHjYGQnwkOCiIrdz9B5O3Xws4iLKMjlouR2s3JX1SA6PTqJd5p2p+c3K4idMZApV5xIn2vPrb1RN0FF8k9BLhclp420KpbxY3zPxlSuXIHX2/TliccmUL5SeVo88QD2iSfwOXXyoq4rIjlz2xJ9KV7CQ4KcznG/0LkeocGBmee5E/8No0bRddyb3Lz3K8I7DuHLaxplu6YOfha5OBqRy0XJ9aDnrPz9YexYBjw2gbO+pZj70XO8vOJtypw9P43ia4xudIpcJI3I5aJlG3nnof1jPehYNpCn182m77YltD6wjWc6DmFz9YakWKsQF7lIGpGL24QGB3Jp+ct4tW0/ej44hhTjw0dzn+XFle9wbRlPVyfivRTk4lYvdqmHv58vW6vVp8Ojk5l5U2f6bP+UJe8OhHXrcn1vzI54WkSupubIpbSIXK1DoUVSKcjFbdJWhyYmJeNrDImXXMr0e4ay/t2PKXuJL7RqxY8PhtH25aXZwjpj66IF4hMSiVgYpzAXQXPk4iZZ92tJTl1RfPrsOY7fdCt88w0HwgZRa+4MpldcTnjHoWyjHhEL4wDSfwBklHEBkZb6S0mmJfriFrntc562ND8qdj/VvvmKN5ZN5Oo/f2dGky6Mvf0hLq9SkcOpI3Fnsu79oqX+Ulxpib54VG4rNtNG1ocTEtlcvSHt+07hg+CO/HvbYpbNHMyVu7fnGOKQfQMvLfWXkkZBLm6R14rNtGkRgNOX+POfdk/wwP2juST5HJ/MeYZnV0dTOulMpveYPK4nUlIoyMUt8jp5KG1uO+NrvrymESF9pzC3cQj9ty5i2awhBMfvAxwLkHIbpWf8waFuFynuFOTiFmkrQQP8/bI9l7Y0P+tqUYC/S5fhuZBB9Or5KqXPneGTOc8wcu1MNg5tTmAOo3wD6Uv91e0iJYFudorbpbUh5tVlkvUGabkzp3l2TTQP7oqFunVZG/EGT3xXKlM3iwF6NavOq6ENiNkRz/D5u9I7ZDLSCUXijXSzU4qM0OBAwkOCqBrgz+GERKJi9zsdIWedajlVugyvdB7KpikfwMmTtO7TlcXxn1KjnG/6fi/jezZOD/GIhXFOQxw0hy7Fi/rIxe2y9pSnTXcAmUbmab/POnpvHhzIpw1uxA5/is4zpzLjyqX8PG4qrXudH2E76zvPSNvlSnGiIBe3y21xT9YpFmcbc8XsiCdi5SES2w5iQY1mjPlsMi0f6sz+NYMImvoGlC6d64jbAG3qVHHZ9xHxNE2tiNvlFLIXOt2R8QfB2lpNCAmbyqJ6dxAUPQmaNIHt23MdcVtgwdfxuuEpxYaCXNwup5C90OmOrIH/16XlCO80lL73vADHj0PTpkQfWEJ5n5Qcr6FFQ1KcKMjF7Zz1lOfndKCcAn//TbfDnj3Qqxd13p3I+k9G0Pr0rzleRzc8pbhQkIvb5et0ISdy/UFQsSK89x4sWUKFk38w6+1BPL/tY0oln8t2naw/ELRwSLyV+sjFK11QL/qJEzBkCHzwAd9eeS3DOw5l7xXXAo7g73FTIGv2HeVwQiIBZfw49c85klLO//egzbekqMmpj1xBLsXe5vEzqP2fpyl/+iSTm/fk7Wb34nfpJZxLsSQl5/7vXwuHpCjRgiApkWJ2xPPoiarc2Xcqy4NaMHzDHBbNHk71wwfyDHHQPLp4BwW5FGtprYoJ/uUZ0iWcx0Kf5V8nj/PfWUMZuGkevik5LxoCCCiTfW8YkaJGQS7FWtYRdWxQc9qFTSX2+lsJXz+bRbOHc/3Rn3J8/x+nk3TjU4o8zZFLsZbbyUQd9m3glZVvc9mZv5nY4kGm3dKDZB/nW+36+RjKXVqKhNNJOk5OPKZQ58iNMe2NMfuNMT8YY0a64poirpDbPujL67SkXdhbrKzdjGfWvc+CD56m9rGfnb42KcXyx+kkbYUrRVKBg9wY4wtMBToANwAPGGNuKOh1RVwhY8+6MyfKVGBQ6EgGdhlB9YQjLJ01mMc3f5Ln3LlWhkpR4ooReVPgB2vtAWvtWeAjoKsLriviEqHBgWwceQcTejbOcXS+tO5ttAubyupaTRn5xSwWfBBOrWO/5HpddbRIUeGKIA8EMv6L/zX1sUyMMf2NMduMMduOHj3qgo8VyZ+8RufHylbkidAInuwczjV//MayWYPp/9UCfHIYnWsrXCkq3Na1Yq39P2ttE2ttkypVtIWoeEba6DzHg5uN4b83tKJd2FusvfYmnl07k4/njODa45n3bPHzNRe8N4xIYXNFkMcD1TL8+erUx0SKrLxG00fLVeSxbs8x5O7h1DrxK8tmDSZsy6L00fm5ZMuweTsztSZqrxbxlAK3HxpjSgHfAW1xBPhW4EFr7Z6c3qP2Q/G0rKcU5abKqRO8FjuFu37YwtbAGwjvOISfKp2fPUzbt2XB1/GZrqe9WsTVCq390Fp7DhgExAJ7gfm5hbhIUZB1vjzHqRbgaLlK9Os+imGdnuL6Y4dYPnMwj25bjLGO/c4Tk5KZ+9UvOZ56JFLYtCBIhMy7Keb2X8QVJ48zJnYKbX/cyldX1+OZjkM4VLFqjq83wMHITi6vV0ombZolkou0m6AHIzvl2NUC8PtllxPW4z883XEodY/+xPKZT/LI1/9NH51npc4WcQcFuUgWOR1ckc4YPmlwJ3eFTeWravV5adU05s59lmoJ/8v2nrTOFt0IlcKkqRURJ5wdXBEVuz/7vi3Wcm/cSkZ9Ph1fm0Jk6z58ENwRHx9fxt3XCIAXl+whITEp22dULOPHC53r6WaoXLCcplZKeaIYkaIuNDjQacAOnbcz8wPG8HHDdmyoEUzkZ5N5ZeU7dNi/iWc6DgEa5doZ88fpJCIWxqV/nsjF0tSKyAXKLWx/K1+FR+59iRHtn6TB/74nNnog378UxT9ns4/EM1Jni7iCglwkH3K7EYoxzGsUQkjYVLZXrUP44onMnjeKwD9/z/Wa2rNFCkpBLpIPuW2Lm+Zw+SsY/Ggkr3cbRuPfviN2xkAe2PkZ5HA/Sp0tUlAKcpF8SFtIlBeLIWjUcLr2f5tdV13HmNgpvD//P1T9K/PoPGNnS0bqcpH8UNeKyEXI7eShNL7GkGwtxqbQa+dnPLt2Bj6+vkzo8BjTarehasUyTk8acrZ9gJb7C2hBkIhLXcgUS3LqIMkaHz4I7sjbkxZyabOmjFz4Jgd3TWFjr+ucBnPagdEZ6aao5EZBLnIR0qZYAvz9Lvg9b/1kYdUqmDIF1q+H+vVhxoxsc+c53fzUTVHJiYJc5CKFBgey84V2TOjZmMAAfwy5d7UkWws+PjBwIMTFQXAwhIVBp07w6/n9znO6+ambopITLQgSKaCsi4dqRSxLn1bJyNeYTCtGA9s9z1u3tKXhlEioX5/tw17gyUsaEv/nP9ne62PQQRaSI43IRVzsgVuqOX282bUViVgYR3zqDou//nWGnpfczIq5Kzh2bR1ufPEpXo6O4IqTx7O9N8XCtkMnCrly8VbqWhEpBM/HxDH3q19IthZfY3jglmqs2XfUaadLYIA/JiWFu1bP55kv3uesbylevPMxFtVrAyb7TumBqXu/qIOl5Mmpa0VBLuImNUcudbrXeVpUW6DGiXiilk3k5vhvWVm7Kc+GDOJouUrZ3qN2xJJJ7YciHpbbTcy0536qFEjPB8fwSpswbvtpJyujB9B1z5psnS2JSckMn79LC4UEUJCLuE1O+5yHhwRlei7Fx5fopt3o2GcSByoFMvHTcUxbNJrKf/+R6b3J1jJ03k56vful276DFE0KchE3yXhOaFqrYtr0SNYzRAEOXH419/R6g9Gt+9L6wNesnD6ALt9+kW10vvHHEzwfE+fmbyNFiebIRYoQZ8vzAWod+4Vxy8bT+LfvWH59c55vN4DjZQPSn/c1hh/HdHR3ueJmmiMX8QLOlucD/Fi5Gj16RxHZqg93/LiFFdED6LR3ffrzydZqg60STEEuUoTktgw/2ceXd5rdQ6c+k/gl4EqmLnmdKTGRVDr9J+DoeolPSCRiYZzCvIRRkIsUIReyDP+HytXp0Xssb9z+MO2+38yK6AG0378x/XltsFXyKMhFipAL2VURHKPzt269j7v7TOBw+Sq8EzOGyYtfp2Lq6DyvLXaleFGQixQhWTtbAvz9qFjGj+zrOx2+q1KD7r3HEnXbQ4R89yUrogcS8t0mDGh6pQRR14qIl8jrMIs6vx9k7LIJ1D/yI4vrtuKdHkNY/ko3N1YohU1dKyJeLqcFRWn2XVGT0IfG8WbLXnTcv4H3x/eFxYvdXaZ4gIJcxEvktKAo4yKic76lmNTiAbo8Mp4/yleG0FDo3RtOaOfE4kz7kYt4kax7n6fJuojop8Dr2BuzguuXvUfK6NGcWLKcke0GsrdJa+2cWAwpyEW8XFoopx1YUTV1m9uuwYHE+PXj/fjKjF48jukLXmHB/o2MOfY40Nzpoc9Zr6HA9w4KcpFiLCp2P/GX16TLI+MZtGkeA7+cT4ufdvLqb8OIatA8PbTb1KnCgq/j00f1aQuLAIW5FyjQHLkx5l5jzB5jTIoxJtudVBEpfGn7s6SdPJRxdWfaStEkXz/G39ab0Iff5M9LyzHlw1EM+XAMl/1ziviEROZs/jnb1gBaWOQ9CnqzczfQHVjnglpE5CI4258lLYSzrhTd/a/adH5kIlNuvY/uu1cTGz2QVge+dnrgBeS+ZYAUHQUKcmvtXmutfmSLeFBOYXs4IdFpy+LZUn6Mvf1huj00jpOly/Lexy/w+rKJXHbm72zXuJAtA8Tz3NZ+aIzpb4zZZozZdvToUXd9rEixl9vJQ85aFiuW8QMg7qrruLvPRKY2u5d7dn9ObPRAbju4PdM12tSpUtjliwvkubLTGLMK+JeTp56z1i5Ofc1a4Glr7QUt19TKThHXcbaHeW5nejp7faPD+xm3dDy1T/zKh41CeK1NGKdKlyEwwJ+NI+9wy/eQvOW0sjPPrhVr7Z2FU5KIuEJO7Yc5dZtkfH18QiIG2FU1iE6PTmLYhjn027KI2w9uZ0SHIWyq0dhdX0MKwCV7rWhELuKdnO3fEhy/j7HLJlDrxK982LgD5SeP5+6WQR6qUDIqlL1WjDHdjDG/ArcCS40xsQW5noi4l7MbpTsC69Cxz0SmNe3O/Ts/I/ju29nwzjwPVCcXqqBdK4ustVdba0tba6+01oa4qjARKXw53Sg941eaMW36ck+vN/jH14+WT9wPAwbAqVNurlAuhDbNEinB8jrIYvvVdenYZxLTm3SFd96Bhg1h7Vr3FSgXREEuUoJlbU/0NdmPsDjjV5qZPQbDunXg6wtt2sCTT8Lf2fvOxTMU5CIlXGhwIBtH3sHByE6Mu6+R0z3Pw0OCoGVL2LULhgyBqVMdo/N1WtRdFCjIRSRdTnuep7cylikDEyacn15p3doR7Bqde5SOehORi/P33zByJEyZArVrw8yZjlG7FBod9SYirlW2LEyeDGvWwLlzcPvt8NRTcPq0pysrcRTkIlIwrVtDXBw88QSMHw+NG8OmTZ6uqkRRkItIwZUr57gB+vnnnD6VSErLlrzbtDttXllOzI54T1dX7CnIRcRlYioGcVvvicxtFEK/rYuYPqE/H02arzAvZApyEXGZqNj9HPcpzXMhg+jV81VKnzvDnPee5tTQ4fDPP54ur9hSkIuIy2Tcu2Vjjca07zuVeQ3vove6eXDjjbBliwerK74U5CLiMln3bjlVugzPtn+SYY9GwsmTcOutEBEBZ854qMLiSUEuIi7jbO8Wfz9fWj3ZG3bvhkcfhchIx+h861YPVVn8KMhFxGVyXRlaoQJMnw7LlsGffzpG5889p9G5C2hlp4i4X0KCY/HQzJlQvz7MmgU33eTpqoo8rewUkaIjIABmzIClS+HECbjlFhg1Cs6e9XRlXklBLiKe07GjY+68Vy949VVo0gS2b/d0VV5HQS4inlWxIrz3HixZAseOOUbnL7yg0Xk+KMhFpGjo3NkxOr//fnj5ZWja1LH/ueRJQS4iRUelSjB7NixeDEeOOKZaXn4ZkpI8XVmRpiAXkaKnSxfYswfuu88xzXLLLY4dFsUpBbmIFE2VKsGcObBwIcTHO9oTR4927H0umSjIRaRo69bNMTrv0QOefx6aNXPMpUs6BbmIFH2VK8PcufDJJ/Dzz47R+ZgxGp2nUpCLiPfo0cMxOu/aFZ59Fpo3h2+/9XRVHqcgFxHvUqUKzJ8P8+bBwYMQHAyvv16iR+cKchHxTvfd5xidd+4MI0dCy5awb1+ml8TsiKdF5GpqjlxKi8jVxfakIgW5iHivK66Ajz92zJ9//73j4OeoKEhOJmZHPBEL44hPSMQC8QmJRCyMK5ZhriAXEe9mjGM16LffQocO8MwzcNttzPtgFYlJyZlempiUTFTsfg8VWngU5CJSPFx5paPnfM4c2LePmZP6E7ZlET4pmcM843F0xUWBgtwYE2WM2WeM+cYYs8gYE+CqwkRE8s0YePBB2LOHbdfdxKg10cz7MIIaJ85Pp2Q9jq44KOiIfCVQ31rbEPgOiCh4SSIiBXTVVRz7YD4jujzN9ccOsXzmYPpuXUyZUobwkCBPV+dyBQpya+0Ka21az89m4OqClyQiUnChN17NrS8O5eGnZrDpmob8Z/W7rFv2MqGXaWolN32B5S68nohIgYQGB7L4tfto+/1XMGsWlQ/sg4YNYdIkSEnxdHkuk2eQG2NWGWN2O/nVNcNrngPOAXNyuU5/Y8w2Y8y2o0ePupkUNI0AAAZwSURBVKZ6EZELYQw88oij77x1axgyBNq0gQMHPF2ZSxT48GVjTB/gMaCttfb0hbxHhy+LiMdY6zj0edgwx2rQ11+HAQPAp+g38RXK4cvGmPbAM0CXCw1xERGPMgb69nXsoHjbbfDkk9C2rWO5v5cq6I+gKcBlwEpjzE5jzDsuqElEpPBVqwbLl8O778LXX0ODBvD22145d17QrpXa1tpq1trGqb8ed1VhIiKFzhj4978do/PmzR1TLHfdBT/95OnK8qXoTwqJiBS26tUhNhamTYMtWxyj82nTHPPpXkBBLiICjtF5//6O0fktt8Djj0NIiOMgiyJOQS4iktE118DKlY758k2boH59mD69SI/OFeQiIlkZ4xiRx8VBkybQr59jZ8VffvF0ZU4pyEVEclKzJqxaBVOnwoYNjtH5jBlFbnSuIBcRyY2Pj6Ob5ZtvHMfKhYVBp04QX3QOqFCQi4hciGuvhdWrYfJk+OILqFcPZs0qEqNzBbmIyIXy8YFBgxyj84YN4dFHHWeGHj7s2bI8+ukiIt6oVi1YuxYmTHCM0uvVg9mzPTY6V5CLiFwMHx/HLoq7djmC/OGHoWtX+O0395fi9k8UESlOrrvOMWc+bpyj/7xePce5oW4cnSvIRUQKytcXnnoKdu6EOnWgd2/o3h2OHEl/ScyOeFpErqbmyKW0iFxNzA7Xdb0oyEVEXCUoCNavh6gox86K9erBRx8Rs/1XIhbGEZ+QiAXiExKJWBjnsjBXkIuIuJKvLzz9tGN0Xrs2REcTFbufxKTkTC9LTEomKna/Sz5SQS4iUhjq1HGsBp03j8N//uP0JYcTXHMQtIJcRKSwlCoFlSpRNcDf6dM5PZ5fCnIRkUIWHhKEv59vpsf8/XwJDwlyyfVLueQqIiKSo9DgQACiYvdzOCGRqgH+hIcEpT9eUApyERE3CA0OdFlwZ6WpFRERL6cgFxHxcgpyEREvpyAXEfFyCnIRES9nrAf2zzXGHAUOFcKlKwPHCuG67qTvUDToOxQN+g6ZXWOtrZL1QY8EeWExxmyz1jbxdB0Foe9QNOg7FA36DhdGUysiIl5OQS4i4uWKW5D/n6cLcAF9h6JB36Fo0He4AMVqjlxEpCQqbiNyEZESR0EuIuLlil2QG2NeMcZ8Y4zZaYxZYYyp6uma8ssYE2WM2Zf6PRYZYwI8XVN+GWPuNcbsMcakGGO8pn3MGNPeGLPfGPODMWakp+u5GMaYGcaY340xuz1dy8UyxlQzxqwxxnyb+u9oiKdryi9jzKXGmC3GmF2p3+GlQvus4jZHbowpb639K/X3g4EbrLWPe7isfDHGtANWW2vPGWNeB7DWjvBwWflijKkLpADTgKettds8XFKejDG+wHfAXcCvwFbgAWvttx4tLJ+MMbcDp4D3rbX1PV3PxTDGXAVcZa3dboy5DPgaCPWmvwtjjAHKWmtPGWP8gA3AEGvtZld/VrEbkaeFeKqygNf9pLLWrrDWnkv942bgak/WczGstXutta45WdZ9mgI/WGsPWGvPAh8BXT1cU75Za9cBJzxdR0FYa3+z1m5P/f1JYC9QOJt5FxLrcCr1j36pvwolj4pdkAMYY0YbY34BegH/8XQ9BdQXWO7pIkqIQOCXDH/+FS8Lj+LIGFMDCAa+8mwl+WeM8TXG7AR+B1ZaawvlO3hlkBtjVhljdjv51RXAWvuctbYaMAcY5NlqncvrO6S+5jngHI7vUeRcyHcQKQhjTDlgATA0y/9tewVrbbK1tjGO/6tuaowplKkurzzqzVp75wW+dA6wDHihEMu5KHl9B2NMH+BuoK0tojcy8vH34C3igWoZ/nx16mPiAanzyguAOdbahZ6upyCstQnGmDVAe8DlN6G9ckSeG2PMdRn+2BXY56laLpYxpj3wDNDFWnva0/WUIFuB64wxNY0xlwD3A0s8XFOJlHqjMBrYa61909P1XAxjTJW0jjNjjD+Om+iFkkfFsWtlARCEo2PiEPC4tdarRlXGmB+A0sDx1Ic2e2HnTTdgMlAFSAB2WmtDPFtV3owxHYEJgC8ww1o72sMl5ZsxZi7QGsf2qUeAF6y10R4tKp+MMS2B9UAcjv+WAZ611i7zXFX5Y4xpCLyH49+SDzDfWvtyoXxWcQtyEZGSpthNrYiIlDQKchERL6cgFxHxcgpyEREvpyAXEfFyCnIRES+nIBcR8XL/D71zoKwdblBEAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aELevxqbEjP1"
      },
      "source": [
        "Pytorch variables, however, are different in at least two aspects.\n",
        "\n",
        "## Autograd\n",
        "\n",
        "First, *PyTorch operations* have *gradient operations* associated with them. \n",
        "If you define a python function composed of PyTorch operations, you can automatically obtain the tensors' gradients. This saves you time as you do not have to define a gradient function. Also, *PyTorch tensors* have a .grad attribute, which accumulates gradients for that variable.\n",
        "\n",
        "So, if you want to compute $\\frac{\\partial}{\\partial W} C $ for the following equation:\n",
        "\n",
        "<center> $ \\displaystyle W_{t+1} = W_{t} - \\gamma \\frac{\\partial}{\\partial W} C $ </center>\n",
        "\n",
        "you can define your $C$ as a python function composed of torch operations, and $W$ is a PyTorch tensor;\n",
        "\n",
        "(more on this: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7btxJG3qx2f",
        "outputId": "203d53f2-971b-4b98-a5f4-9ac03faa58ec"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import grad\n",
        "\n",
        "w = torch.tensor(3, dtype=float, requires_grad=True)\n",
        "result = torch.pow(w, 2.0)\n",
        "\n",
        "print(\"This is also in numpy:\")\n",
        "print(\"w:\", w)\n",
        "print(\"w^2:\", result)\n",
        "\n",
        "print(\"\\nThis is not in numpy:\")\n",
        "# the gradient function d(result)/dw\n",
        "print(\"The output of the gradient function:\", grad(outputs=result, inputs=w))\n",
        "\n",
        "\n",
        "# Alternatively you can do this:\n",
        "w = torch.tensor(3, dtype=float, requires_grad=True)\n",
        "loss = torch.pow(w, 2.0)\n",
        "loss.backward()\n",
        "# the accumulated gradient after the backward pass\n",
        "print(\".grad after backward:\", w.grad)\n",
        "# now for each tensor with requires_grad=True\n",
        "# grad(outputs=loss, inputs=tensor) will be called and accumulated in the .grad attribute"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is also in numpy:\n",
            "w: tensor(3., dtype=torch.float64, requires_grad=True)\n",
            "w^2: tensor(9., dtype=torch.float64, grad_fn=<PowBackward0>)\n",
            "\n",
            "This is not in numpy:\n",
            "The output of the gradient function: (tensor(6., dtype=torch.float64),)\n",
            ".grad after backward: tensor(6., dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq5JGB0WvErW"
      },
      "source": [
        "## CUDA variables\n",
        "\n",
        "Second, PyTorch tensors may be stored in the CPU memory (RAM) or GPU memory. When performing an operation, all tensors used in the process must be stored in the same place. Otherwise, an error will be raised. If they are stored in the GPU, you will use this unit to perform the computation. Otherwise, the CPU will be used. The same goes for machines with multiple GPUs. The *.to* method can be used to copy variables from different devices. Devices are called \"CPU\" for the CPU and \"cuda:n\" for the nth GPU.\n",
        "\n",
        "A useful trick is to write your code with a **device** variable. In this way, if you want to run in CPU or GPU, you can simply change this variable. Another useful trick is that, for models, *model.to* works for all parameters of the network.\n",
        "\n",
        "(more on this: https://pytorch.org/docs/stable/notes/cuda.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbtUUPPjFQ00",
        "outputId": "d6c5faf0-1b70-4885-9fee-907c9e742145"
      },
      "source": [
        "import torch\n",
        "x = torch.tensor(1)\n",
        "y = torch.tensor(2)\n",
        "print(x+y)\n",
        "\n",
        "x = x.to(\"cuda:0\")\n",
        "y = y.to(\"cuda:0\")\n",
        "print(x+y)\n",
        "\n",
        "# Some useful code (if you share a machine with your collegues)\n",
        "GPU_TO_USE=\"0\"\n",
        "device = f\"cuda:{GPU_TO_USE}\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", device)\n",
        "x = x.to(device)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3)\n",
            "tensor(3, device='cuda:0')\n",
            "DEVICE: cuda:0\n",
            "tensor(1, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LPIiNQfPUNv"
      },
      "source": [
        "## The non-existent fit method\n",
        "\n",
        "We will now use pytorch to solve the previous example but using gradient descent instead of an analytic solution.\n",
        "\n",
        "**Homework:**\n",
        " * Can you solve this example but using a GPU?\n",
        "\n",
        "**Optimizer Code:**\n",
        "```\n",
        "opt = torch.optim.SGD([w], learning_rate)  # Initialize the optimizer with your parameters\n",
        "(...)\n",
        "opt.zero_grad()   # Zero-out the gradients of the parameters;\n",
        "(...)             # Use the data and model to compute your loss value;\n",
        "error.backward()  # Compute the gradients by backprogation;\n",
        "opt.step()        # Perform gradient descent;\n",
        "```\n",
        "\n",
        "\n",
        "**from (CUDA and requires_grad=True) to numpy Code:**\n",
        "```\n",
        "w  # torch tensor\n",
        "w.detach()  # torch tensor without gradient and past operations\n",
        "w.cpu()   # torch tensor copied to cpu\n",
        "w.numpy()    # numpy array\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "XjESAptFPvmx",
        "outputId": "96a755ac-b254-4909-ab69-830f932c26d8"
      },
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# generate toy data\n",
        "n = 100\n",
        "d = 1\n",
        "trans = torch.randn(d)\n",
        "error = torch.randn(n) * 0.1\n",
        "X = torch.randn(n, d)\n",
        "y = torch.matmul(X, trans) + error\n",
        "\n",
        "plt.scatter(X.numpy(), y.numpy())\n",
        "\n",
        "# analitic\n",
        "XX_inv = torch.inverse(torch.matmul(X.T, X))\n",
        "Xy = torch.matmul(X.T, y)\n",
        "w_analitic = torch.matmul(XX_inv, Xy)\n",
        "\n",
        "print(w_analitic)\n",
        "axis_x = torch.linspace(-3, 3, 50)\n",
        "plt.plot(axis_x, w_analitic * axis_x, c=\"r\")\n",
        "\n",
        "# gradient descent\n",
        "iterations = 10\n",
        "learning_rate = 0.25\n",
        "w_desc = torch.randn(1, requires_grad=True)\n",
        "opt = torch.optim.SGD([w_desc], learning_rate)\n",
        "loss_history = []\n",
        "\n",
        "for i in range(iterations):\n",
        "  # optimization\n",
        "  opt.zero_grad()\n",
        "  output = torch.matmul(X, w_desc)\n",
        "  loss = torch.mean((y-output)**2)\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "  # saving results\n",
        "  loss_history.append(loss.detach().cpu().numpy().copy())\n",
        "\n",
        "plt.plot(axis_x, (w_desc * axis_x).detach(), ls=\"--\", c=\"g\")\n",
        "plt.figure()\n",
        "plt.plot(loss_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0066])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe5c3ac3410>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRkLcS-JYjXH"
      },
      "source": [
        "If you are familiar with Keras and Sklearn you know that to optimize a model you just call something like this:\n",
        "```\n",
        "model.fit(X, y)\n",
        "```\n",
        "\n",
        "Internally, Keras and Sklearn (when optimizing MLPs) are doing something similar to the previous example. In PyTorch, this looks something like this:\n",
        "\n",
        "```\n",
        "def train_epoch():\n",
        "    # lists that will save the training progression of the model  \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # model in train mode - change the behaviour of dropout and batchnorm layers\n",
        "    model.train()\n",
        "\n",
        "    # run the training loop for all batches in epoch\n",
        "    for X, y in train_loader:\n",
        "      \n",
        "      X, y = X.to(device), y.to(device)  # data to device (gpu if using one)\n",
        "      opt.zero_grad()                    # set gradients to zero\n",
        "      y_ = model(X)                      # compute model output  \n",
        "      loss = loss_fn(y_, y)              # compute training loss for batch\n",
        "      loss.backward()                    # autograd/backprop magic!\n",
        "      opt.step()                         # run one step of gradient descend\n",
        "      loss = loss.detach().cpu().numpy() # reads - remove_gradient().to_cpu().to_numpy_array()\n",
        "      train_losses.append(loss)          # save current loss\n",
        "\n",
        "    # model in eval mode - change the behaviour of dropout and batchnorm layers\n",
        "    model.eval()\n",
        "\n",
        "    # computation within this block does not require gradients (we are in validation)\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # run validation loop for all batches in epoch \n",
        "      for X, y in val_loader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_ = model(X)\n",
        "        loss = loss_fn(y_, y)           # notice we do not call backward in validation\n",
        "        loss = loss.detach().cpu().numpy()\n",
        "        val_losses.append(loss)\n",
        "    return np.mean(train_losses), np.mean(val_losses)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(epoch, *train_epoch())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkx-pk4NM0o"
      },
      "source": [
        "## Example 1: CIFAR10 Classification with Deep Neural Networks\n",
        "\n",
        "We finish the basics with an exercise. Use the provided examples below to define your data, model, loss function, and optimizer. Complete the code below and train your classifier in PyTorch for the CIFAR10 dataset. You will find that although you can reinvent the wheel in PyTorch, usually you do not need to do so.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=147m8A-V__JRsFA68K3Lo-WvOyu9S2D3d\" width=\"550\" hspace=\"30\">\n",
        "</center>\n",
        "\n",
        "## Data\n",
        "For loading and processing image data, the Torchvision package is handy. It contains most of the transformations you will ever use. Take a look here:\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "You can also use Torchvision to load \"famous\" datasets or to load your images directly from a folder. Alternatively, you may implement your own Dataset. We will look at this in a second, but further information can be found here:\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "Finally, the dataloader object handles things like multi-threading, batch creation, and data shuffling, etc. Most of the time, you will use the Pytorch dataloader, but there is useful information in the previous link if you want to look at it.\n",
        "\n",
        "**Why do we use batches and what is the batch size?**\n",
        "Training would require a lot more time if, for each gradient descent step we would need to feed all the training inputs to the model. By providing only a small batch of data in each step, we guaranty we have a sufficiently good approximation of the gradient, with a much smaller computational cost. This is the difference between classic gradient descent and stochastic gradient descent.\n",
        "\n",
        "**Transform object - useful for standarization/data augmentation**\n",
        "```\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize([(0.5, 0.5, 0.5),\n",
        "                                                      (0.5, 0.5, 0.5)])\n",
        "```\n",
        "**Dataset object**\n",
        "```\n",
        "# torchvision dataset\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# load from folder\n",
        "\"\"\"\n",
        " path should look like this\n",
        "dog/xxx.png\n",
        "dog/xxy.png\n",
        "dog/xxz.png\n",
        "cat/123.png\n",
        "cat/nsdf3.png\n",
        "cat/asd932_.png\n",
        "\"\"\"\n",
        "dataset = torchvision.datasets.ImageFolder(path, transform=transform)\n",
        "\n",
        "# custom dataset - here you define how to read your data\n",
        "dataset = myDataset(<your arguments>)\n",
        "```\n",
        "\n",
        "**Train, Validation and Test Split**\n",
        "\n",
        "You can use the utils.data module to split your dataset in train, validation and test. This way you can correctly evaluate your model in a subset of unseen data. Other types of division exist (e.g. K-fold validation) but for now this will suffice.\n",
        "```\n",
        "torch.utils.data.random_split(dataset, [train_length, val_length, test_length])\n",
        "```\n",
        "\n",
        "\n",
        "**Dataloader**\n",
        "```\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Model\n",
        "When building a model in PyTorch, **torch.nn** is your best friend:\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "It contains all layers you will use in your work except if you are researching something new, yet to be implemented. Layers, sub-layers, and models extend the nn.Module class. For this, you need to implement the **forward** method, which is called when you do something like \"model(X)\" or \"model.conv1(X)\", and returns the result of the operation. Weights are initialized at layer level. If you build a custom one, you are responsible by initializing the weights.\n",
        "\n",
        "The example below shows how to implement a simple model. In this case, we use the nn.Sequential module which implements the **forward** method by composing all layers sequentially. Later we will write our custom model in PyTorch by extending nn.Module.\n",
        "\n",
        "```\n",
        "model = nn.Sequential(nn.Conv2d(3, 6, 5),nn.ReLU(),nn.MaxPool2d(2, 2),\n",
        "                      nn.Conv2d(6, 16, 5),nn.ReLU(),nn.MaxPool2d(2, 2),\n",
        "                      nn.Linear(16 * 5 * 5, 120),nn.ReLU(),\n",
        "                      nn.Linear(120, 84),nn.ReLU(),\n",
        "                      nn.Linear(84, 10))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Optimizer\n",
        "\n",
        "The torch.optim package implements several optimizers used to train deep neural networks. The two most famous examples are probably stochastic gradient descent with momentum and the Adam algorithm.\n",
        "```\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "```\n",
        "```\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "```\n",
        "\n",
        "## Loss function\n",
        "\n",
        "The torch.nn module also implements several loss functions for you to use in your models:\n",
        "```\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "```\n",
        "```\n",
        "loss_fn = torch.nn.MSELoss()  # the mean squarred error is not used for classification!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv4E1MsRckdC",
        "outputId": "e2bb8cf0-2517-443b-d708-1618ae18886a"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as pl\n",
        "\n",
        "GPU_TO_USE=\"0\"\n",
        "device = f\"cuda:{GPU_TO_USE}\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "model = nn.Sequential(\n",
        "          nn.Conv2d(3, 6, 5),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2),\n",
        "          nn.Conv2d(6, 16, 5),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2, 2),\n",
        "          nn.Flatten(),\n",
        "          nn.Linear(16 * 5 * 5, 120),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(120, 84),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(84, 10)\n",
        "        )\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "print(model)\n",
        "\n",
        "def train_epoch():\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # used for accuracy computation\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        " \n",
        "    print(\"\\nTrain\")\n",
        "    model.train()    \n",
        "    for i, (X, y) in enumerate(train_loader):\n",
        "      print(\"\\rtrain: {:.0%}\".format(i/len(train_loader)), end=\"\", flush=True)\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y_ = model(X)\n",
        "      loss = loss_fn(y_, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      train_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "      # compute valus for accuracy\n",
        "      _, predicted = torch.max(y_.data, 1)\n",
        "      train_correct += (predicted == y).sum().item()\n",
        "      train_total += y.size(0)\n",
        "      \n",
        " \n",
        "    print(\"\\nValidation\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for i, (X, y) in enumerate(val_loader):\n",
        "        print(\"\\rvalid: {:.0%}\".format(i/len(val_loader)), end=\"\", flush=True)\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_ = model(X)\n",
        "        loss = loss_fn(y_, y)\n",
        "        val_losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        _, predicted = torch.max(y_.data, 1)\n",
        "        val_correct += (predicted == y).sum().item()\n",
        "        val_total += y.size(0)\n",
        "\n",
        "    return  np.mean(train_losses), np.mean(val_losses),\\\n",
        "    train_correct/train_total,val_correct/val_total\n",
        " \n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss, valid_loss, train_acc, val_accuracy = train_epoch()\n",
        "    print(f\"\\nEpoch: {epoch}\\n\",\n",
        "          f\"\\ttrain loss: {train_loss}\\n\",\n",
        "          f\"\\tvalid loss: {valid_loss}\\n\",\n",
        "          f\"\\ttrain acc: {train_acc*100}%\\n\",\n",
        "          f\"\\tvalid acc {val_accuracy*100}%\",)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Sequential(\n",
            "  (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (8): ReLU()\n",
            "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (10): ReLU()\n",
            "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 0\n",
            " \ttrain loss: 1.846841812133789\n",
            " \tvalid loss: 1.5745843648910522\n",
            " \ttrain acc: 32.46%\n",
            " \tvalid acc 42.46%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 1\n",
            " \ttrain loss: 1.500867247581482\n",
            " \tvalid loss: 1.4127689599990845\n",
            " \ttrain acc: 45.32%\n",
            " \tvalid acc 48.54%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 2\n",
            " \ttrain loss: 1.3823989629745483\n",
            " \tvalid loss: 1.3429486751556396\n",
            " \ttrain acc: 49.746%\n",
            " \tvalid acc 51.239999999999995%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 3\n",
            " \ttrain loss: 1.3124996423721313\n",
            " \tvalid loss: 1.2789822816848755\n",
            " \ttrain acc: 52.442%\n",
            " \tvalid acc 53.800000000000004%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 4\n",
            " \ttrain loss: 1.254752516746521\n",
            " \tvalid loss: 1.2479231357574463\n",
            " \ttrain acc: 54.632000000000005%\n",
            " \tvalid acc 55.730000000000004%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 5\n",
            " \ttrain loss: 1.1972087621688843\n",
            " \tvalid loss: 1.1911407709121704\n",
            " \ttrain acc: 57.036%\n",
            " \tvalid acc 57.199999999999996%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 6\n",
            " \ttrain loss: 1.148061990737915\n",
            " \tvalid loss: 1.1559197902679443\n",
            " \ttrain acc: 58.821999999999996%\n",
            " \tvalid acc 58.879999999999995%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 7\n",
            " \ttrain loss: 1.1110410690307617\n",
            " \tvalid loss: 1.1463377475738525\n",
            " \ttrain acc: 60.221999999999994%\n",
            " \tvalid acc 58.730000000000004%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 8\n",
            " \ttrain loss: 1.0759334564208984\n",
            " \tvalid loss: 1.123190999031067\n",
            " \ttrain acc: 61.622%\n",
            " \tvalid acc 59.650000000000006%\n",
            "\n",
            "Train\n",
            "train: 99%\n",
            "Validation\n",
            "valid: 98%\n",
            "Epoch: 9\n",
            " \ttrain loss: 1.04458749294281\n",
            " \tvalid loss: 1.0942492485046387\n",
            " \ttrain acc: 62.926%\n",
            " \tvalid acc 60.980000000000004%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olHy4knVf8I-"
      },
      "source": [
        "Below are some examples simple to obtain in PyTorch, which may help your research or even debug potential problems with your model. Due to PyTorch's NumPy-like nature, investigating weights, activations, gradients, etc., is straightforward.\n",
        "\n",
        "Notice that we can convert these tensors to NumPy and plot them with matplotlib. This enables you to really look at your model during training and after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x7CaB-znf8nj",
        "outputId": "44e52811-3eb1-4f19-fd6d-5553668c99c5"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "print(\"\\n Weights\")\n",
        "weights = []\n",
        "for layer in model:\n",
        "  if hasattr(layer, \"weight\"):\n",
        "    weights.append(layer.weight)\n",
        "  else:\n",
        "    weights.append(None)\n",
        "\n",
        "for i, w in enumerate(weights):\n",
        "  if w!= None:\n",
        "    print(i, w.shape, w.min(), w.max(), w.mean(), w.var())\n",
        "  else:\n",
        "    print(i, w)\n",
        "plt.imshow(weights[0][0, 0].cpu().detach())\n",
        "\n",
        "print(\"\\n Activations\")\n",
        "for i, (X, y) in enumerate(train_loader):\n",
        "  break\n",
        "x = X.to(device)\n",
        "for i, layer in enumerate(model):\n",
        "  x = layer(x)\n",
        "  print(i, x.shape, x.min(), x.max(), x.mean(), x.var())\n",
        "\n",
        "# view gradients\n",
        "print(\"\\n Gradients\")\n",
        "for i, (X, y) in enumerate(train_loader):\n",
        "  X, y = X.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  y_ = model(X)\n",
        "  loss = loss_fn(y_, y)\n",
        "  loss.backward()\n",
        "  break\n",
        "\n",
        "grads = []\n",
        "for layer in model:\n",
        "  if hasattr(layer, \"weight\"):\n",
        "    grads.append(layer.weight._grad)\n",
        "  else:\n",
        "    grads.append(None)\n",
        "\n",
        "for i, g in enumerate(grads):\n",
        "  if g!= None:\n",
        "    print(i, g.shape, g.min(), g.max(), g.mean(), g.var())\n",
        "  else:\n",
        "    print(i, g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Weights\n",
            "0 torch.Size([6, 3, 5, 5]) tensor(-1.1652, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.4681, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.0116, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.1493, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "1 None\n",
            "2 None\n",
            "3 torch.Size([16, 6, 5, 5]) tensor(-0.9129, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.9452, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.0321, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.0507, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "4 None\n",
            "5 None\n",
            "6 None\n",
            "7 torch.Size([120, 400]) tensor(-0.4768, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.5064, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.0108, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.0083, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "8 None\n",
            "9 torch.Size([84, 120]) tensor(-0.4186, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.3568, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.0233, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.0102, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "10 None\n",
            "11 torch.Size([10, 84]) tensor(-0.6101, device='cuda:0', grad_fn=<MinBackward1>) tensor(0.3567, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.0035, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.0248, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "\n",
            " Activations\n",
            "0 torch.Size([4, 6, 28, 28]) tensor(-11.9102, device='cuda:0', grad_fn=<MinBackward1>) tensor(11.0671, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.3169, device='cuda:0', grad_fn=<MeanBackward0>) tensor(3.2839, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "1 torch.Size([4, 6, 28, 28]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(11.0671, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.4816, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.3763, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "2 torch.Size([4, 6, 14, 14]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(11.0671, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.7595, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.9817, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "3 torch.Size([4, 16, 10, 10]) tensor(-32.7730, device='cuda:0', grad_fn=<MinBackward1>) tensor(18.4132, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-3.3281, device='cuda:0', grad_fn=<MeanBackward0>) tensor(34.8147, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "4 torch.Size([4, 16, 10, 10]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(18.4132, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.6672, device='cuda:0', grad_fn=<MeanBackward0>) tensor(2.8104, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "5 torch.Size([4, 16, 5, 5]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(18.4132, device='cuda:0', grad_fn=<MaxBackward1>) tensor(1.3782, device='cuda:0', grad_fn=<MeanBackward0>) tensor(5.5100, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "6 torch.Size([4, 400]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(18.4132, device='cuda:0', grad_fn=<MaxBackward1>) tensor(1.3782, device='cuda:0', grad_fn=<MeanBackward0>) tensor(5.5100, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "7 torch.Size([4, 120]) tensor(-34.7247, device='cuda:0', grad_fn=<MinBackward1>) tensor(13.1163, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-5.9822, device='cuda:0', grad_fn=<MeanBackward0>) tensor(53.2890, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "8 torch.Size([4, 120]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(13.1163, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.7691, device='cuda:0', grad_fn=<MeanBackward0>) tensor(4.2612, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "9 torch.Size([4, 84]) tensor(-13.2809, device='cuda:0', grad_fn=<MinBackward1>) tensor(6.3665, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-2.3987, device='cuda:0', grad_fn=<MeanBackward0>) tensor(9.4488, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "10 torch.Size([4, 84]) tensor(0., device='cuda:0', grad_fn=<MinBackward1>) tensor(6.3665, device='cuda:0', grad_fn=<MaxBackward1>) tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.8534, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "11 torch.Size([4, 10]) tensor(-6.8172, device='cuda:0', grad_fn=<MinBackward1>) tensor(5.4755, device='cuda:0', grad_fn=<MaxBackward1>) tensor(-0.3049, device='cuda:0', grad_fn=<MeanBackward0>) tensor(7.7757, device='cuda:0', grad_fn=<VarBackward0>)\n",
            "\n",
            " Gradients\n",
            "0 torch.Size([6, 3, 5, 5]) tensor(-0.1164, device='cuda:0') tensor(0.1527, device='cuda:0') tensor(-0.0145, device='cuda:0') tensor(0.0018, device='cuda:0')\n",
            "1 None\n",
            "2 None\n",
            "3 torch.Size([16, 6, 5, 5]) tensor(-0.1074, device='cuda:0') tensor(0.0969, device='cuda:0') tensor(-0.0003, device='cuda:0') tensor(0.0005, device='cuda:0')\n",
            "4 None\n",
            "5 None\n",
            "6 None\n",
            "7 torch.Size([120, 400]) tensor(-0.1654, device='cuda:0') tensor(0.1302, device='cuda:0') tensor(-0.0003, device='cuda:0') tensor(6.9724e-05, device='cuda:0')\n",
            "8 None\n",
            "9 torch.Size([84, 120]) tensor(-0.1377, device='cuda:0') tensor(0.1246, device='cuda:0') tensor(-0.0009, device='cuda:0') tensor(7.5232e-05, device='cuda:0')\n",
            "10 None\n",
            "11 torch.Size([10, 84]) tensor(-0.2190, device='cuda:0') tensor(0.0975, device='cuda:0') tensor(-2.3416e-09, device='cuda:0') tensor(0.0007, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJhklEQVR4nO3d34tchR2G8fd1sqmaVIzUizSbNl5Y2yCtwpIKuUsR4g/0qqCgV0IuqhCLINo7+wdYoXgTVCxoFUEvRCwSMFakVl01WmMUgigmSFMJNiaxidm8vdgppJLNnpmcs2fny/OBhZ2d5cybzT57dmeXGScRgDrO63sAgHYRNVAMUQPFEDVQDFEDxazo4qCDVasyteaSLg7dvgm789+n+l4wmlPnT9AH+KT7XtDYyUOHNHf06BkHdxL11JpLtP7O33Zx6NZ5gv4jJWnFsb4XjOboT4/3PaGxwZcr+57Q2IGH/rDgdXz7DRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFNMoattbbX9se5/t+7oeBWB8i0ZteyDpYUnXSdoo6VbbG7seBmA8Tc7UmyTtS/JJkhOSnpZ0c7ezAIyrSdTrJH1+2uX9w7f9H9vbbM/anp07erStfQBG1NodZUl2JJlJMjNYtaqtwwIYUZOoD0haf9rl6eHbACxDTaJ+S9Llti+zvVLSLZKe73YWgHEt+mD+SU7avkvSS5IGkh5LsqfzZQDG0ugZOpK8KOnFjrcAaAF/UQYUQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDGNHiRhVBlEJ9bMdXHo1p13fLK+rg0mbO9Fa471PaGxw99O0Md2kAWvmqB/BYAmiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWIWjdr2Y7YP2v5gKQYBODdNztSPS9ra8Q4ALVk06iSvSjq0BFsAtICfqYFiWova9jbbs7Zn544cbeuwAEbUWtRJdiSZSTIzWL2qrcMCGBHffgPFNPmV1lOSXpd0he39tu/ofhaAcS36DB1Jbl2KIQDawbffQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0Us+iDJIzjvKlTWr32SBeHbt0/fvnnvieMZPfx431PGMkf//mrvic09vrsz/ue0JhPeMHrOFMDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQzKJR215ve5ftD23vsb19KYYBGE+Txyg7KemeJO/Y/r6kt23vTPJhx9sAjGHRM3WSL5K8M3z9a0l7Ja3rehiA8Yz0M7XtDZKulvTGGa7bZnvW9uzc4WPtrAMwssZR214t6VlJdyc5/N3rk+xIMpNkZnDRhW1uBDCCRlHbntJ80E8mea7bSQDORZN7vy3pUUl7kzzY/SQA56LJmXqzpNslbbG9e/hyfce7AIxp0V9pJXlN0sLP8QFgWeEvyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKbJ436PbOWBaP3vvu3i0K372a9/0/eEkaz4pu8Fo/nhX7/ue0JjF192qu8Jja34z8LXcaYGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKWTRq2+fbftP2e7b32H5gKYYBGE+ThzM6LmlLkiO2pyS9ZvsvSf7e8TYAY1g06iSRdGR4cWr4ki5HARhfo5+pbQ9s75Z0UNLOJG90OwvAuBpFnWQuyVWSpiVtsn3ld9/H9jbbs7ZnT8wda3sngIZGuvc7yVeSdknaeobrdiSZSTKzcnBhW/sAjKjJvd+X2r54+PoFkq6V9FHXwwCMp8m932sl/cn2QPNfBJ5J8kK3swCMq8m93+9LunoJtgBoAX9RBhRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMU0e+WRkcxes0L+vvKSLQ7fuR7//W98TRjLY+JO+J4zkm/UX9T2hsaNrJ+ccNze18HWT868A0AhRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxTSO2vbA9ru2X+hyEIBzM8qZerukvV0NAdCORlHbnpZ0g6RHup0D4Fw1PVM/JOleSacWegfb22zP2p799viRVsYBGN2iUdu+UdLBJG+f7f2S7Egyk2Rm6nurWxsIYDRNztSbJd1k+1NJT0vaYvuJTlcBGNuiUSe5P8l0kg2SbpH0cpLbOl8GYCz8nhooZqSn3UnyiqRXOlkCoBWcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKMZJ2j+o/S9Jn7V82B9I+rLlY3ZpkvZO0lZpsvZ2tfXHSS490xWdRN0F27NJZvre0dQk7Z2krdJk7e1jK99+A8UQNVDMJEW9o+8BI5qkvZO0VZqsvUu+dWJ+pgbQzCSdqQE0QNRAMRMRte2ttj+2vc/2fX3vORvbj9k+aPuDvrcsxvZ627tsf2h7j+3tfW9aiO3zbb9p+73h1gf63tSE7YHtd22/sFS3ueyjtj2Q9LCk6yRtlHSr7Y39rjqrxyVt7XtEQycl3ZNko6RrJN25jD+2xyVtSfILSVdJ2mr7mp43NbFd0t6lvMFlH7WkTZL2JfkkyQnNP/PmzT1vWlCSVyUd6ntHE0m+SPLO8PWvNf/Jt67fVWeWef974vOp4cuyvpfX9rSkGyQ9spS3OwlRr5P0+WmX92uZfuJNMtsbJF0t6Y1+lyxs+K3sbkkHJe1Msmy3Dj0k6V5Jp5byRichanTM9mpJz0q6O8nhvvcsJMlckqskTUvaZPvKvjctxPaNkg4meXupb3sSoj4gaf1pl6eHb0MLbE9pPugnkzzX954mknwlaZeW930XmyXdZPtTzf/IuMX2E0txw5MQ9VuSLrd9me2Vmn/i++d73lSCbUt6VNLeJA/2vedsbF9q++Lh6xdIulbSR/2uWliS+5NMJ9mg+c/Zl5PcthS3veyjTnJS0l2SXtL8HTnPJNnT76qF2X5K0uuSrrC93/YdfW86i82Sbtf8WWT38OX6vkctYK2kXbbf1/wX+p1JluzXRJOEPxMFiln2Z2oAoyFqoBiiBoohaqAYogaKIWqgGKIGivkvmOgDCfci6BQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intermediate Topics\n",
        "---\n"
      ],
      "metadata": {
        "id": "DuBHSU6yySq4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUzKbXSAygFs"
      },
      "source": [
        "\n",
        "## Custom Datasets\n",
        "\n",
        "To write a custom Dataset in Pytorch, you need to create a class that extends the torch.utils.data.Dataset class and implement two methods:\n",
        "\n",
        "* \\_\\_len__ so that len(dataset) returns the dataset's size.\n",
        "* \\_\\_getitem__ to support the indexing such that dataset[i] can be used to get the ith sample\n",
        "\n",
        "Let's look at the following very simple example:\n",
        "\n",
        "```\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, type, transform, K, fold):\n",
        "        #load file with images/labels\n",
        "        self.X, self.Y = pickle.load(open(f'k{K}.pickle', 'rb'))[fold][type]\n",
        "        #get transforms\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        #iterate over each image/label during train/test cycles\n",
        "        #apply transforms\n",
        "        X = self.transform(self.X[i])\n",
        "        Y = self.Y[i]\n",
        "        return X, Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "```\n",
        "\n",
        "Although this example may seem trivial, we will actually use it later for our classification experiment.\n",
        "\n",
        "It is common for Datasets to have a transform attribute that determines how to process data in the input pipeline. This allows you to rotate, resize, normalize, etc., the input data. The transform attribute is often a **torchvision.transforms.Compose** object with many sub transforms. Much like the **nn.Sequential** class enables you to have a pipeline that sequentially processes the data. You can also create your transforms.\n",
        "\n",
        "In this pipeline, it is widespread to first transform the data to a PIL image, use the PIL image functions for data augmentation, and finally convert it to a tensor and normalize it. Take a look at the following example:\n",
        "\n",
        "```\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import random\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    Random90kRot(),\n",
        "    transforms.Resize((224, 224)), \n",
        "    transforms.RandomCrop(128),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "class Random90kRot():\n",
        "    \"\"\"Rotate by one of the given angles with equal probability.\"\"\"\n",
        "    def __init__(self, angles=[0, 90, 180, 270]):\n",
        "        self.angles = angles\n",
        "\n",
        "    def __call__(self, x):\n",
        "        angle = random.choice(self.angles)\n",
        "        return TF.rotate(x, angle, fill=(0,))\n",
        "```\n",
        "\n",
        "About the Python Image Library or Pillow: this is a library that enables you to open, manipulate, and save images with python. Although it is not used directly in our examples, some transforms from Torchvision are based on this package. You may look at this tutorial if this framework is of interest to you:\n",
        "\n",
        "https://pillow.readthedocs.io/en/stable/handbook/tutorial.html\n",
        "\n",
        "The package intersects with similar packages such as ScikitImage and Opencv, although some functions may be present in only some of them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation - Albumentations\n",
        "\n",
        "```\n",
        "#or using Albumentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(224, 224), \n",
        "    A.RandomCrop(128, 128),\n",
        "    A.HorizontalFlip(),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "2i-CCDP9zGQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Models\n",
        "\n",
        "The torchvision package provides you well-known pretained models, which you can use for different tasks, including **classification**, **segmentation** and **object detection**. Generally, pretrained models require less time to optimize to new tasks and usually perform better for small and medium-sized datasets. Often, the last layer of the models are substituted for a new one. We can optimize the last few layers of the model (fine-tuning), or the whole model. Fine-tuning is based on the idea that early layers contain general/low-level features which are useful for a multitude of tasks, while the \"later\" ones are more specific to the task which the model is trying to solve.\n",
        "\n",
        "To look deeper into the available models: https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "For classification, well-known models include:\n",
        "\n",
        "```\n",
        "import torchvision.models as models\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "```\n",
        "\n",
        "Notice that these models were trained for images of size $224\\times 224$."
      ],
      "metadata": {
        "id": "R7aILaI-zUd9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7dzHylvykpg"
      },
      "source": [
        "## Custom Layers/Models;\n",
        "\n",
        "\n",
        "Writing custom models is easy in PyTorch.\n",
        "You need to:\n",
        "\n",
        "\n",
        "1.   Extend the nn.Module class\n",
        "2.   Implement the \\_\\_init__ method\n",
        "3.   Implement the forward method\n",
        "\n",
        "You can also use nn.Module's inside your nn.Module. This is how available architectures are implemented in PyTorch.\n",
        "\n",
        "Let's look at the following example:\n",
        "\n",
        "```\n",
        "from torch import nn\n",
        "\n",
        "class Model_example(nn.Module):\n",
        "    def __init__(self, pretrained_model, n_outputs):\n",
        "        super().__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        model = getattr(models, pretrained_model)(pretrained=True)\n",
        "        #remove last layer from pre-trained model \n",
        "        model = nn.Sequential(*tuple(model.children())[:-1])\n",
        "        #get last dimension of the model\n",
        "        last_dimension = torch.flatten(model(torch.randn(1, 3, 224, 224))).shape[0]\n",
        "        self.model = nn.Sequential(\n",
        "            model,\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(last_dimension, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "```\n",
        "\n",
        "By doing this you can use functions like:\n",
        "* model(x)  # forward\n",
        "* torch.save(model.state_dict(), path)  # save model\n",
        "* model.load_state_dict(torch.load(path))  # load model\n",
        "* model.parameters()  # for the optimizer for example\n",
        "* model.train()\n",
        "* model.eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2 - Multiclass Classification model for Cervical Cancer risk assessment;\n",
        "\n",
        "Now let's look at the following example.\n",
        "It is based on this code and covers the main topics on a classification task:\n",
        "\n",
        "https://github.com/tomealbuquerque/ordinal-losses\n",
        "\n",
        "## Target\n",
        "![](https://drive.google.com/uc?export=view&id=1ppNapl6i1a11ZgzLNZyoFTlncCmeNw7i)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1EUH2MJgtmUj7_JccGns_I7k5zw_zQ1vU)\n"
      ],
      "metadata": {
        "id": "jMkxxeDjNDsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#package to import google drive files to colab\n",
        "!pip install PyDrive"
      ],
      "metadata": {
        "id": "joWqohueNHgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0608de7-8681-41b7-b61f-701b316a07e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.10)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.17.3)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.54.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (21.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (57.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':\"1G90PIYhDFOpYdAtKG70_0OWmvwuVSHD2\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('k7.pickle')"
      ],
      "metadata": {
        "id": "FSZxCLLyNTQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4839cecc-d024-4ed4-a5e3-966f6c28056a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 44, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    \"file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\"\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhkwJV85dRmQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "ee1c86c531eb4dd89f7afd4cb7b7beec",
            "c50a1fcf1fd94091876df8a0383d2cf4",
            "aa6360310a314786957014de31e72cfb",
            "b2317f5a7be341daa29ceb8900c5a956",
            "6db541361bff482995e619d0e32e49b8",
            "eca6bcdab5ba4307be1245b3134179da",
            "b219d60e10924c17ad1572b12b3a867e",
            "ef7361a3564843059647697c1d779d40",
            "8138263adf7344279c40a19c8bd3ed38",
            "31c29ab3b6d04ade977188acce17bf9f",
            "317f6cc092434b9485982c87ab203642"
          ]
        },
        "outputId": "d6341149-3e61-4fde-c6dd-7d7f38ed1d7b"
      },
      "source": [
        "#Main pipeline for multiclass classification\n",
        "#import libraries\n",
        "import numpy as np\n",
        "from time import time\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "from torchvision import models, transforms\n",
        "import pickle\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#hyperparameters to tune\n",
        "#number of classes \n",
        "K = 7\n",
        "#fold (in this case we have 3 k-folds) \n",
        "fold = 0\n",
        "#number of images per mini-batch\n",
        "batchsize=64\n",
        "#epochs to finish the train \n",
        "epochs=10\n",
        "#learning rate\n",
        "lr=1e-4\n",
        "#pre-trained architecture\n",
        "architecture='mobilenet_v2'\n",
        "\n",
        "#Prepare dataset loader\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, type, transform, K, fold):\n",
        "        self.X, self.Y = pickle.load(open(f'k{K}.pickle', 'rb'))[fold][type]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        X = self.transform(self.X[i])\n",
        "        Y = self.Y[i]\n",
        "        return X, Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "#train transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomAffine(180, (0, 0.1), (0.9, 1.1)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(saturation=(0.5, 2.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "#val transforms\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "#dataloaders\n",
        "tr_ds = MyDataset('train', train_transforms, K, fold)\n",
        "tr = DataLoader(tr_ds, batchsize, True)#random selection of images - True\n",
        "ts_ds = MyDataset('test', val_transforms, K, fold)\n",
        "ts = DataLoader(ts_ds, batchsize)\n",
        "\n",
        "#loss\n",
        "ce = nn.CrossEntropyLoss()\n",
        "\n",
        "# Model\n",
        "class Base(nn.Module):\n",
        "    def __init__(self, pretrained_model, n_outputs):\n",
        "        super().__init__()\n",
        "        self.n_outputs = n_outputs\n",
        "        model = getattr(models, pretrained_model)(pretrained=True) #using transfer learning from a pretrained model - True\n",
        "        model = nn.Sequential(*tuple(model.children())[:-1])\n",
        "        last_dimension = torch.flatten(model(torch.randn(1, 3, 224, 224))).shape[0]\n",
        "        self.model = nn.Sequential(\n",
        "            model,\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(last_dimension, 512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def loss(self, Yhat, Y):\n",
        "        return ce(Yhat, Y)\n",
        "\n",
        "    def to_proba(self, Yhat):\n",
        "        return F.softmax(Yhat, 1)\n",
        "\n",
        "    def to_classes(self, Phat):\n",
        "        return Phat.argmax(1)\n",
        "\n",
        "#test cycle\n",
        "def test(val):\n",
        "    model.eval()\n",
        "    val_avg_acc = 0\n",
        "    with torch.no_grad():\n",
        "      for X, Y in tqdm(val):\n",
        "          X = X.to(device)\n",
        "          Y = Y.to(device, torch.int64)\n",
        "          Yhat = model(X)\n",
        "          Khat = model.to_classes(model.to_proba(Yhat))\n",
        "          val_avg_acc += (Y == Khat).float().mean() / len(val)\n",
        "    return val_avg_acc\n",
        "\n",
        "#train cycle\n",
        "def train(tr, val, epochs=epochs, verbose=True):\n",
        "    for epoch in range(epochs):\n",
        "        if verbose:\n",
        "            print(f'* Epoch {epoch+1}/{epochs}')\n",
        "        tic = time()\n",
        "        model.train()\n",
        "        avg_acc = 0\n",
        "        avg_loss = 0\n",
        "        for X, Y in tqdm(tr):\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device, torch.int64)\n",
        "            opt.zero_grad()\n",
        "            Yhat = model(X)\n",
        "            loss = model.loss(Yhat, Y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            Khat = model.to_classes(model.to_proba(Yhat))\n",
        "            avg_acc += (Y == Khat).float().mean() / len(tr)\n",
        "            avg_loss += loss / len(tr)\n",
        "        dt = time() - tic\n",
        "        out = ' - %ds - Loss: %f, Acc: %f' % (dt, avg_loss, avg_acc)\n",
        "        if val:\n",
        "            model.eval()\n",
        "            out += ', Test Acc: %f' % test(val)\n",
        "        if verbose:\n",
        "            print(out)\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "#get output probabilities from the model\n",
        "def predict_proba(data):\n",
        "    model.eval()\n",
        "    Phat = []\n",
        "    with torch.no_grad():\n",
        "        for X, _ in data:\n",
        "            phat = model.to_classes(model.to_proba(model(X.to(device))))\n",
        "            Phat += list(phat.cpu().numpy())\n",
        "    return Phat\n",
        "\n",
        "#Give to the model the architecture and number of classes (K)\n",
        "model = Base(architecture, K)\n",
        "#put the model on 'GPU' or 'CPU'\n",
        "model = model.to(device)\n",
        "#Define optimizer and scheduler \n",
        "opt = optim.Adam(model.parameters(), lr)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee1c86c531eb4dd89f7afd4cb7b7beec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Model"
      ],
      "metadata": {
        "id": "HGbQtyZNN_bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Start the training process\n",
        "train(tr, ts)"
      ],
      "metadata": {
        "id": "o4bjNCpFN-ao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb2c19b-625c-4fbb-faea-d05cf27e1eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.20it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 8s - Loss: 0.514571, Acc: 0.806071, Test Acc: 0.672125\n",
            "* Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.15it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 8s - Loss: 0.527410, Acc: 0.787857, Test Acc: 0.604875\n",
            "* Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.14it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 8s - Loss: 0.508951, Acc: 0.791205, Test Acc: 0.667375\n",
            "* Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:09<00:00,  1.08it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 9s - Loss: 0.488510, Acc: 0.806830, Test Acc: 0.682875\n",
            "* Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.29it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.440283, Acc: 0.825312, Test Acc: 0.667250\n",
            "* Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.41it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.432930, Acc: 0.808661, Test Acc: 0.684125\n",
            "* Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.435513, Acc: 0.837545, Test Acc: 0.658875\n",
            "* Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.377504, Acc: 0.870357, Test Acc: 0.681875\n",
            "* Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.307256, Acc: 0.886473, Test Acc: 0.666875\n",
            "* Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
            "100%|██████████| 5/5 [00:00<00:00,  7.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - 7s - Loss: 0.292446, Acc: 0.877634, Test Acc: 0.678000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluate Model Performance"
      ],
      "metadata": {
        "id": "V2ZHcfshNqi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate model performance\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred=predict_proba(ts)\n",
        "_,y_true = pickle.load(open(f'k{K}.pickle', 'rb'))[fold]['test']\n",
        "print('Classification Report:\\n', classification_report(y_true, y_pred))\n",
        "\n",
        "print('Confusion Matrix:')\n",
        "cm = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, cmap='Blues')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cse-6Wu9NXNN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "1b5e412a-6a48-4da1-cbc3-0a49e53fa931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      1.00      0.91        25\n",
            "           1       1.00      0.74      0.85        23\n",
            "           2       0.70      0.85      0.77        33\n",
            "           3       0.68      0.84      0.75        61\n",
            "           4       0.44      0.40      0.42        48\n",
            "           5       0.68      0.55      0.61        66\n",
            "           6       0.67      0.64      0.65        50\n",
            "\n",
            "    accuracy                           0.68       306\n",
            "   macro avg       0.71      0.71      0.71       306\n",
            "weighted avg       0.68      0.68      0.67       306\n",
            "\n",
            "Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU1fnH8c8zM4mQBMKaASUQICiyuFCkUkQ2QQSqIIiiVWyBVCrWrVXQVosVhVq1WqsVRKWtilTl54JaLYKCKBgU2a2ACGGZsAQIS7aZ8/tjJjFokknIzNyT4Xn7ui/m3pm598swPjk5955zxRiDUkqp6HE5HUAppeKdFlqllIoyLbRKKRVlWmiVUirKtNAqpVSUeaJ9gFte32jlZQ3Th3Z0OoJS6nvqeZDa7qP+uZOqXXOOffFErY9XHVEvtEopVVeJyFYgH/ADJcaY7iLSBHgZyAC2AqONMXlV7Ue7DpRS8UVc1V+qp58x5hxjTPfQ+mRgoTGmA7AwtF4lLbRKqfjicld/OTGXAXNCj+cAw8NGOtEjKaWUlUSqv4RngPdEZKWIZIW2eY0xu0KPdwPecDvRPlqlVHypfpcAoeKZVW7TTGPMzHLrFxhjdohIGvC+iGws/35jjBGRsCfftNAqpeJL9VqqAISK6swqnt8R+jNXROYDPQCfiLQ0xuwSkZZAbrjjaNeBUiq+ROhkmIgki0iD0sfAIGAt8AYwNvSyscDr4SJpi1YpFV9q0KINwwvMl+D+PMCLxph3ReQzYJ6IjAO+BUaH25EWWqVUfDnxqwmOY4zZApxdwfZ9wICa7EsLrVIqvtTgZFisaKFVSsWXyHUdRIx9pT+kUT0PN/4kncn923Jnv7Zc2K4xAIPPaMYfBrXnt30z+G3fDM5MS3Y058dLPuLSoRczbPBAZs+q9ORlzNmaC+zNprlqxtZcURgZVmvWtmgDxvD6ulxyDhZyisfF7X0y+GrPEQA+3JzHos37HU4Ifr+fB6bdx9OznsPr9XL1laPo268/7TMzNVcdy6a54iMXYGXXgX2JQg4V+sk5WAhAYUkAX34hqfXs+rmwds1q0tPb0Co9nYTERAYPGcriRQudjmVtLrA3m+aKj1wAuN3VX2IkbKEVkY4icqeIPB5a7hSRM2MRrlST+gm0Sq3Ht3kFAPRu15g7+mYw5pwW1E9w7mdFrs9Hi5YtytbTvF58Pp9jeUrZmgvszaa5asbWXECkh+BGRJVVSkTuBOYCAqwILQK8JCKVzlgjIlkiki0i2Wv+M69WARPdws97nMb8tT4KSwIs3ZrHH9/fzEOLt3KwsIThndNqtX+lVJypg32044DOxpji8htF5BFgHTC9ojeVH9ZWm4m/XQK/6HEaK3MOsnrXYQAOF/rLnv9060EmnN/qRHdfa2leL7t37S5bz/X58HrDzi8RdbbmAnuzaa6asTUXUCevOggAp1awvWXouagac25LfPlFLN783Zy6DU/5rl+la8sUdh0qjHaMSnXu0pVt27aSk7Od4qIi3n17AX369Xcsj+25wN5smis+cgF1skV7C7BQRL4Gtoe2tQYygUnRDNa2SX3OS09l58ECfts3A4C31u+hW6uGnJZ6ChjYf7SYeV/urnpHUeTxeJhy9z1MzBpPIOBn+IiRZGZ2cCyP7bnA3myaKz5yAVa2aMWYqn+zFxEXwRlrTgtt2gF8ZozxV/6u7+g9w5RS1RWRe4Zd8mj17xn2zq123DPMGBMAPo1BFqWUqj0Lr6O168JUpZSqLQu7DrTQKqXii7ZolVIqyrTQKqVUlEVoPtpI0kKrlIov2kerlFJRpl0HSikVZdqiVUqp6BIttEopFV0nZaH9zYXton2IE/LWul1OR6jQ4I4twr/IAR63fV9epSoiLvu+q9qiVUrFlZOyRauUUrGkhVYppaJMC61SSkWbfXVWC61SKr5oi1YppaLM5dKRYUopFVXaolVKqWizr85qoVVKxRdt0SqlVJRpoVVKqSjTIbi1cPXwi0lKTsLlcuN2u3nq+ZcdyfHqkzP46vNPSE5txM0PPw/A3EensmfnNgAKjh6mXlIKNz0025F8pabecxdLPlxMkyZNmTf/TUezfN/HSz5ixvRpBPwBRoy8gnETspyOBGiumrI1l7Zoa+nhvz1LaqPGjmbo1ncw5w8ewSt/e6Bs21W33lv2+O1/PEm9pGQnoh3np5eOYPRV13Dv3ZOdjnIcv9/PA9Pu4+lZz+H1ern6ylH07def9pmZmktzRUSkC62IuIFsYIcxZpiItAXmAk2BlcC1xpiiqvZh3wVnlmvb6WySUhpU+JwxhrWfLOKsXgNinOqHunU/j9TUVKdj/MDaNatJT29Dq/R0EhITGTxkKIsXLXQ6luaKk1wQLLTVXarpZmBDufUZwKPGmEwgDxgXbgd1ptCKCHf8+pfcMHY0b/3fv52OU6GtG1aTnNqYZi1bOR3FWrk+Hy1afjcVZJrXi8/nczBRkOaqGVtzQWQLrYi0AoYCz4TWBegPvBJ6yRxgeLj9nHChFZGfV/Fclohki0j2C88/c6KHOM5fnp7D0/+Yx4OPPsXrr8xl9RfZEdlvJK3+eCFnW9CaVeqkJtVfyteq0PL9jua/AHcAgdB6U+CAMaYktJ4DnBYuUm1atFMre8IYM9MY090Y0/2a68fX4hDfaZ7mBaBxk6Zc0GcAG9evjch+I8XvL2HdiiV0/Uk/p6NYLc3rZfeu3WXruT4fXq/XwURBmqtmbM0FwSG41V3K16rQMrN0PyIyDMg1xqysdaaqnhSR1ZUsa4CYfarHjh3l6JEjZY+zVywjo53zne7lbV6zkuantia1aZrTUazWuUtXtm3bSk7OdoqLinj37QX06dff6ViaK05yQUS7DnoBl4rIVoInv/oDjwGNRKT0QoJWwI5wOwp31YEXuJhgh+9xfxdgWbidR0re/n3ce+ctQPBs54BBQ+jR84JYHf44L//lPrasX8XR/IPMuGEUA0b/nO79h7L64w84q5cdXzSAu+64jezszzhwII9LLurDL391E8MvH+V0LDweD1PuvoeJWeMJBPwMHzGSzMwOTsfSXHGSC4jYEFxjzBRgCoCI9AV+Y4y5RkT+DYwiWHzHAq+HjWSMqfxJkdnAc8aYpRU896Ix5upwB8jJK6r8AA76dNs+pyNUSO8Zpk5m9Ty1L5Otb3qj2jVn218vrdbxyhXaYSLSjmCRbQJ8AfzMGFNY1furbNEaYyq9bKE6RVYppWItGgMWjDGLgcWhx1uAHjV5f50asKCUUuHoyDCllIoynetAKaWiTFu0SikVZVpolVIqyiyss1polVLxRVu0SikVZS49GaaUUtFlYYNWC61SKr6clC3aZg0So32IEzKsc0unI1Ro/NwvnY5QoWeuOtvpCJUqKPY7HaFO8bgsnobaU/siqS1apZSKMj0ZppRSUWZhndVCq5SKLy4Lu0a00Cql4oq2aJVSKsq0j1YppaLMwjqrhVYpFV+0RauUUlFmYZ3VQquUii8n5cgwpZSKJe06UEqpKLOwztadQvvxko+YMX0aAX+AESOvYNyELKcjAfbkapKUwA0/aU1qPQ8GWPT1Pv7z1V5aN67HL3q0IsHtwm8Mz6/IYcu+Y45kLGXLZ1aeb/cu/vC7KezfvxdBGD5yNFddc63TsazNBTD1nrtY8uFimjRpyrz5bzodp4y2aE+Q3+/ngWn38fSs5/B6vVx95Sj69utP+8xMzRUSMIYXP9/J1v3HqOdx8cchp7Nmdz5jzj2V19b4WL0zn7NPbcCYbqcy7f3NMc9XyqbPrDy328PNt99BxzM7ceTIEcaOGUWP83vSrr3mqsxPLx3B6Kuu4d67Jzsd5TgW1lnsG6tWgbVrVpOe3oZW6ekkJCYyeMhQFi9a6HQsq3IdOFbC1v3BlmpBSYCdBwtoUj8BA9RPcAOQlOgm72ixI/lK2fSZldeseXM6ntkJgOTkZDLatWNPbq7DqezNBdCt+3mkpqY6HeMHXC6p9hKzTOFeICIdRWSAiKR8b/vg6MU6Xq7PR4uWLcrW07xefD5frA5fKVtzNUtOoE2T+mzed5R/Ze9gTLeWPDbiTMZ0O5WXV+1yNJutn1l5O3fs4H8bN9C561lORzmOrblsIyLVXmKlykIrIr8GXgduAtaKyGXlnn6givdliUi2iGTPnjUzMklVtZzicXHzhRn8K3snx4oDDDi9KS9k7+Tm+Rt4IXsHE85Pdzqi1Y4ePcLk39zMrb+dQkpKSvg3xIituWxkY6EN10c7AfiRMeawiGQAr4hIhjHmMaDSlMaYmcBMgIISTG1Dpnm97N61u2w91+fD6/XWdre1Zlsut8DNF2awbGse2dsPAtC7XRP+mb0TgOXbDjLe4UJr22dWXklxMZNvv4XBQ4bRb8BAp+OUsTWXrepiH63LGHMYwBizFegLXCIij1BFoY20zl26sm3bVnJytlNcVMS7by+gT7/+sTp8nck1vmc6Ow8W8M6GvWXb8o4Vc6Y3GYDOLVLYnV/oVLxgBss+s1LGGO6f+nsy2rbj6muvdzpOGVtz2awutmh9InKOMWYVQKhlOwx4Fuga9XQhHo+HKXffw8Ss8QQCfoaPGElmZodYHb5O5Dq9eTK92zVhW94xpg05HYB5q3Yx+9Mcru1+Ki6XUOwPMHt5jiP5Stn0mZX35arPeeetN8jscDo/Gz0CgIk33UKv3n00VyXuuuM2srM/48CBPC65qA+//NVNDL98lNOxrGzRijGV/2YvIq2AEmPM7gqe62WM+TjcASLRdXAy0XuG1ZzeM6xmbL5nWMoptS+TA/76SbVrzsKbesakLFfZojXGVNr8qU6RVUqpWHNZ2KStEwMWlFKquiyss1polVLxRYfgKqVUlFk4S2LdGIKrlFLVFakhuCJST0RWiMiXIrJORKaGtrcVkeUisklEXhaRxLCZIvR3U0opK0gN/gujEOhvjDkbOAcYLCLnAzOAR40xmUAeMC7cjrTQKqXiikuqv1TFBB0OrSaEFgP0B14JbZ8DDA+b6YT/NkopZaGajAwrPy9LaMn63r7cIrIKyAXeBzYDB4wxJaGX5ACnhcukJ8OUUnGlJhcdlJ+XpZLn/cA5ItIImA90PJFMWmiVUnElGgMWjDEHRGQR0BNoJCKeUKu2FbAj3Pu10FrmiZFdnI5QocbnTXI6QqVWvfMnpyNUqE2zJKcjnJQiNaG3iDQHikNFtj4wkOCJsEXAKGAuMJbgVLJV0kKrlIorEWzQtgTmiIib4PmsecaYt0RkPTBXRO4HvgBmh9uRFlqlVFyJVNeBMWY1cG4F27cAPWqyLy20Sqm4YuHAMC20Sqn4onMdKKVUlNk414EWWqVUXInlbcSrSwutUiquaNeBUkpFmYUNWi20Sqn4oi1apZSKMvvKrBZapVSccVvYd1Bnpkn8eMlHXDr0YoYNHsjsWZVOthNzNuby7d7FxPHXc+Xlw7jq8p8y94V/Oh2JjQum8tm8u/h07mSWvnAHAJdfdC4rX7mbIysfp1un1g4nhNfn/Ysbx45k0vWjeGjqZIoKC52OBNj5HQN7c9VkmsRYqROF1u/388C0+3jy788w/40FvPv2W2zetMnpWNbmcrs93Hz7Hbz82lvM/udcXnn5RbZsdj7X4KzHOP+q6VxwTXASmHWbd3LV7bNY+vlmh5PBvj25vPnqSzwy8wWeeP4VAoEASz74j9OxrP2O2ZoLgnMdVHeJlTpRaNeuWU16ehtapaeTkJjI4CFDWbxoodOxrM3VrHlzOp7ZCYDk5GQy2rVjT26uw6l+6KtvfHz9rT25An4/RYWF+EtKKCwsoEmz5k5HsvY7ZmsuCM51UN0lZpnCvUBEeojIeaHHnUTkNhEZEv1o38n1+WjRskXZeprXi8/ni2WECtmaq7ydO3bwv40b6Nz1LEdzGGN488lJfPzCHfzi8l6OZqlI0+ZpDL/qOsaNvoSxlw8kOTmFc8/r6XQsa79jtuaCOtiiFZF7gceBp0TkQeAJIBmYLCJ3V/G+sttD2NR3c7I5evQIk39zM7f+dgopKSmOZhnw80f5ydUzGD7pSX55ZW96dWvvaJ7vO5x/iOVLFzNr7ls8/9p7FBQcY9F7C5yOpU6AjX204a46GEXw7o+nALuBVsaYQyLyZ2A5MK2iN5W/PURBCaa2IdO8Xnbv2l22nuvz4fV6a7vbWrM1F0BJcTGTb7+FwUOG0W/AQKfjsHPPQQD25B3mjQ9Wc17nDD62oG+21Krs5XhbnkpqoyYA9Ozdn41rv6TfoKGO5rL1O2ZrLgC3hdfRhus6KDHG+I0xR4HNxphDAMaYY0Ag6ulCOnfpyrZtW8nJ2U5xURHvvr2APv36x+rwdS6XMYb7p/6ejLbtuPra652OQ1K9RFKSTil7fFHPjqzbvNPhVMdr7m3BV+vXUFhwDGMMX36+gvQ2bZ2OZe13zNZcELm74EZSuBZtkYgkhQrtj0o3ikgqMSy0Ho+HKXffw8Ss8QQCfoaPGElmZodYHb7O5fpy1ee889YbZHY4nZ+NHgHAxJtuoVfvPo7kSWvagJcfmQCAx+3m5XeyeX/ZBi7tdxaP3HkFzRqn8NrjN7D6qx1ceuPfHMl4Rqeu9OpzEbdMuBq32027zI5c/NORjmQpz9bvmK25wM4huGJM5b/Zi8gpxpgfXEwoIs2AlsaYNeEOEImug5NJQbHf6QgVavmTm52OUCm9Z1j8qOep/cCu29/8qto15+GfnhGTslxli7aiIhvavhfYG5VESilVCza2aHUIrlIqrlh4LkwLrVIqvngsrLRaaJVSccXCOquFVikVX2I5tLa6tNAqpeKKhXVWC61SKr7oVQdKKRVlNk78rYVWKRVXLKyzWmiVUvFFLLxrmBZay5T47RyxPO8fv3c6QqU+2W7nIMVTEtKcjlCheglupyNUqkXDhFrvQ1u0SikVZVpolVIqymI5oXd1aaFVSsUVt4V3QtRCq5SKKzaODLOw9iul1ImL1B0WRCRdRBaJyHoRWSciN4e2NxGR90Xk69CfjcNmisxfTSml7BDBu+CWALcbYzoB5wM3ikgnYDKw0BjTAVgYWq+SFlqlVFxxIdVeqmKM2WWM+Tz0OB/YAJwGXAbMCb1sDjA8XCbto1VKxZVodNGKSAZwLsG7f3uNMbtCT+0Gwt7+VwutUiqueGpwIa2IZAFZ5TbNNMbM/N5rUoBXgVuMMYfKXz5mjDEiEnaUkRZapVRcqUmLNlRUZ1b2vIgkECyyLxhjXgtt9olIS2PMLhFpCeSGO4720Sql4opLpNpLVSTYdJ0NbDDGPFLuqTeAsaHHY4HXw2WqMy3aj5d8xIzp0wj4A4wYeQXjJmSFf1MM2JirsLCQGydcR3FRESV+P/0GDGL8DZMcy/Py36azfuUyUlIb89tH55RtX/r2q3z87nxcLhdn/qgnw66dGNNcC2b+mU2rlpPUsBETps8CwPftZt597jGKC46R2rwFl06czClJyTHN9X2H8w/x6INT2bplEyLCbXdNpVPXsx3NVMrv95N13ZU0T0tj+qNPOh0HiGgfbS/gWmCNiKwKbbsLmA7ME5FxwLfA6HA7qhOF1u/388C0+3h61nN4vV6uvnIUffv1p31mpuaqQGJiIo///VmSkpIpKS5m4rhrOb9Xb7o49D9n936D6XXJCF766wNl2zat/Zx1ny3l9oefxZOQSP7BvJjn6nrhIH408DLefPpPZdvefuYRBlydReszz+bLD9/l0wX/ps8V18c8W3lP/eVPdD+/F79/4GGKi4spLDjmaJ7yXpn7L9q0bcfRI4edjlImUr+mG2OWQqWXJgyoyb7qRNfB2jWrSU9vQ6v0dBISExk8ZCiLFy10Opa1uUSEpFArrKSkhJKSEkenjmvf6RySUhoet23Zf16n34hr8CQkAtAgNew13xHXuuNZ1EtpcNy2vN05pHc8C4C2Xbrx1WdLYp6rvCOH81mzaiWDfzoCgISEBFIaNAzzrtjI9e3m06UfMeyykU5HOU6kug4imqmmbxCRf0QjSFVyfT5atGxRtp7m9eLz+WId4wdszQXB1vbYMZczbGBvzju/J527nuV0pOPs3bWdbzas5rHJv+TJe25i26YNTkcCoFmrDL5euQyAjcs/In//Hkfz7N65g9RGjXl42j38auxoHn3wDxQcO+poplJPPDKDG359G2LZdFl1rtCKyBvfW94ELi9dr+J9WSKSLSLZs2dVekJPRZHb7WbOS68x/50PWL92DVs2fe10pOP4/X6OHj7Erx/8O8Ouncg/H7kXY5yfi3fohNtZ+d83eO53v6Kw4Bguj7O9a36/n03/28iwEVfw5Jx51KtXn5f/+ayjmQCWLVlMo8ZNOOPMzk5H+QGpwRIr4b5FrYD1wDOAIZitO/BwVW8qf8lEQQm1/r8nzetl967dZeu5Ph9eb9hrhKPO1lzlNWjQkG7de/DpsqW0y+zgdJwyjZo2p+uPL0REaN2hEy5xceTQQVJSGzmaq+mprRkzeQYA+3blsHnVckfzNEvz0ry5l46dg7+RXNBvIPMsKLRrv/yCZUsWs3zZEooKCzly5Aj3//5OfvfHGU5Hs/IuuOG6DroDK4G7gYPGmMXAMWPMh8aYD6MdrlTnLl3Ztm0rOTnbKS4q4t23F9CnX/9YHb7O5crL209+/iEACgsK+Gz5J7TJaOtwquN1Pq83m9Z+AcCendspKSkmuWGqw6ngSOiknAkEWPb6C5w7YJijeZo0bUYzr5ft324FYFX2clq3bedoJoCsSbfyyoKFvPzGe9zzwEN0O6+HFUUWgucoqrvESpUtWmNMAHhURP4d+tMX7j3R4PF4mHL3PUzMGk8g4Gf4iJFkWtA6szXXvr17uP/euwj4AwRMgP4XXUyvC/s6ludfj05l87ovOJJ/kD9mjWTQlT+nR/8hzHtyOg/dOhaPx8NVk+6K+YTN//fENLZtWM2xwwd54qYx9B55HUUFx1j532Cv2BndL+CsCy+OaaaK3HjrZGZMnUJJcTEtTm3F7Xff53Qkq9l4hl9q0i8mIkOBXsaYu6r7nkh0HZxMDheUOB2hQp98s8/pCJXaV1DodIQK9W2n9wyrqRYNE2r90/bfq3ZWu+Zccc6pMfnpXqPWqTFmAbAgSlmUUqrW9FY2SikVZTZ2HWihVUrFFW3RKqVUlNlXZrXQKqXijFtbtEopFV0W1lkttEqp+OLkBEqV0UKrlIor2qJVSqkoC3d3WydooVVKxRVt0ao6q2fbphQUB5yOUaHcgwVOR6jQpY8tdTpChZ7/RQ+nI1SqRcOEWu8jlvPMVpcWWlUtthZZpb7PsnnIAS20Sqk4o1cdKKVUlFnYc6CFVikVX7RFq5RSUaZ9tEopFWV61YFSSkWZfWVWC61SKs5oi1YppaLMvjKrhVYpFW8srLRaaJVScUW7Dmrh4yUfMWP6NAL+ACNGXsG4CVlORwLszFVYWMiNE66juKiIEr+ffgMGMf6GSU7HKnP18ItJSk7C5XLjdrt56vmXHcnx94fv44vlS2nYqDEPzQxm+PSj//LKP2eyc/tW/vj487Q/vVPMcyV6XDz3ix+R6HHhdgn/XZfLk4u2AHDTgPYM7JxGwMC8FTm8uHx7TLPZ+pmVZ1+ZrSOF1u/388C0+3h61nN4vV6uvnIUffv1p31mpuaqQGJiIo///VmSkpIpKS5m4rhrOb9Xb7p0PdvRXOU9/LdnSW3U2NEMfQYN4+JLR/PkQ/eWbUvPaM9t9/yJZx5/0LFcRSUBxj//OceK/Hhcwpzx3Vn69V7aNk+mRWo9LvvrJxgDTZJrPwFLTdn6mR3Hwkpr4515f2DtmtWkp7ehVXo6CYmJDB4ylMWLFjody9pcIkJSUjIAJSUllJSUWDlaxmlndu1GSoOGx207rXVbTk3PcCZQOceK/AB43ILHJRhg9Hmt+PviLRgTfM3+I8Uxz2XzZ1ZKavBfrNSoRSsiFwA9gLXGmPeiE+mHcn0+WrRsUbae5vWyZvXqWB2+UrbmgmBr+xc/u4Id27dx+egxdO56ltORyogId/z6l4jAsBFXMGz4FU5Hso5LYO4NP6Z1k/rMXZHDmpxDpDepz+AuXvqfmUbe0SKmL/iKbfuPOR3VOhZ20VbdohWRFeUeTwCeABoA94rI5CrelyUi2SKSPXvWzIiFVdXndruZ89JrzH/nA9avXcOWTV87HanMX56ew9P/mMeDjz7F66/MZfUX2U5Hsk7AwOinljPw4aV0adWQzLRkEt0uCksCjHl6Ba9m7+C+Ec72hdpKarDESriug/KdQFnAQGPMVGAQcE1lbzLGzDTGdDfGdI/EyaE0r5fdu3aXref6fHi93lrvt7ZszVVegwYN6da9B58us2cS6uZpwc+ocZOmXNBnABvXr3U4kb3yC0r47Js8enVoiu9QIQvX5wKwcMMeOngbOJzOTiJS7aUa+3pWRHJFZG25bU1E5H0R+Tr0Z9iTDeEKrUtEGotIU0CMMXsAjDFHgJKwKSOkc5eubNu2lZyc7RQXFfHu2wvo069/rA5f53Ll5e0nP/8QAIUFBXy2/BPaZLR1OFXQsWNHOXrkSNnj7BXLyGjn7MlD2zROSqBBvWCv3ikeFz3bN+GbPUf5YOMezmvbBIDuGY35dt8RJ2NaS6T6SzU8Dwz+3rbJwEJjTAdgYWi9SuH6aFOBlQRb2UZEWhpjdolICjFseXs8HqbcfQ8Ts8YTCPgZPmIkmZkdYnX4Opdr39493H/vXQT8AQImQP+LLqbXhX2djgVA3v593HvnLUCwH3nAoCH06HmBI1kef/BuNqxeSf7BA9x4zVBGXZtFSoOGPP/knzl0MI8//f5WMtqfzpQH/hrTXM0anML9l3fGLcFrQv+zzsdH/9vLF9sO8OCoLlz7k9YcLSrhD/+3Iaa5wN7PrLxIFiZjzEcikvG9zZcBfUOP5wCLgTurzGRKT2HWgIgkAV5jzDfhXltQQs0PcBI7XBCzXxRqxOZb2dh6z7Drn10R/kUOsPmeYd0yGta6Tn65Pb/aNeec1g1/SbBbtNRMY8xxJ5ZChfYtY0yX0PoBY0yj0GMB8krXK3NC19EaY44CYYusUkrFWk0u2woV1RM+Y2+MMSIStrDXiWUhKpoAAAlcSURBVOtolVKquiLcR1sRn4i0DB5LWgK54d6ghVYpFVdiUGjfAMaGHo8FXg/3Bi20Sqm4EsmRYSLyEvAJcIaI5IjIOGA6MFBEvgYuCq1XqU7MdaCUUtUVyZFhxpgxlTw1oCb70UKrlIorFo7A1UKrlIozFlZaLbRKqbiiE38rpVSU2VdmtdAqpeKNhZX2hIbg1oQOwa0ZW4fgqprzHSx0OkKFbnrVjjmTK/LBr3vWukx+7TtW7ZrTwVs/JmVZW7RKqbhiYRetFlqlVHyxsM5qoVVKxZfqTOgda1polVJxxcI6q4VWKRVfLKyzWmiVUnHGwkqrhVYpFVdqMvF3rGihVUrFFe2jVUqpKHNpoVVKqWizr9JqoVVKxRXtOqiFj5d8xIzp0wj4A4wYeQXjJmSFf1MM2JirsLCQGydcR3FRESV+P/0GDGL8DZOcjgXYm83WXABvvfoi7y+YD8Zw0dAR/HTUNY7kSHALj43sQoJbcLuEDzftY87yHO4alMkZ3hRKAoaNuw/zyKIt+APOTXFiYZ2tG4XW7/fzwLT7eHrWc3i9Xq6+chR9+/WnfWam5qpAYmIij//9WZKSkikpLmbiuGs5v1dvunQ929FcNmezNde332zi/QXz+dOT/8CTkMAf75xE9569aXla65hnKfYbbpu/joLiAG6X8Piozqz49gALv9rLA+9tAuB3F3dgaOc03ljji3m+Uja2aOvEzRnXrllNenobWqWnk5CYyOAhQ1m8aKHTsazNJSIkJSUDUFJSQklJiTWXvNiazdZcO779htPP7MIp9erjdnvodPaP+HTJB47lKSgOAOBxCR6XYAws//ZA2fMbfYdplpLoVDwg+G9Z3SVWqiy0IvJjEWkYelxfRKaKyJsiMkNEUmMTEXJ9Plq0bFG2nub14vM59xOzlK25INjaHjvmcoYN7M155/ekc9eznI5UxtZsNuZq3bY969d8Qf7BAxQWHOPz5UvZm+vcd8wlMHPMWbw2vjvZ2w6y0Xe47Dm3SxjYsRmflSu8TpAaLLESrkX7LHA09PgxIBWYEdr2XGVvEpEsEckWkezZs2ZGJKiqGbfbzZyXXmP+Ox+wfu0atmz62ulIZWzNZmOuVm3aMeKq65l6x6/4452TaNv+DFwu534RDRjIemk1o59dSccWKWQ0qV/23C1927J6Rz5rduY7lg+CXQfVXWIlXB+tyxhTOhN1d2NMt9DjpSKyqrI3GWNmAjMhMhN/p3m97N61u2w91+fD6/XWdre1Zmuu8ho0aEi37j34dNlS2mV2cDrOcWzNZluui4YM56IhwwH41zN/pWlz579jR4r8rMo5RI82jdi6/xjX9WhFav0EHvngK6ejWdHl833hfjSuFZGfhx5/KSLdAUTkdKA4qsnK6dylK9u2bSUnZzvFRUW8+/YC+vTrH6vD17lceXn7yc8/BEBhQQGfLf+ENhltHU4VZGs2W3MBHMjbD8Ae3y6WL1nEhQMucSRHan0PyYluABLdLn6Unsq2vGMM6ZzGeW0acf+7X9txOxUL+w7CtWjHA4+JyO+AvcAnIrId2B56LiY8Hg9T7r6HiVnjCQT8DB8xkkwLWhq25tq3dw/333sXAX+AgAnQ/6KL6XVhX6djAfZmszUXwEN/+A35hw7idnuYcPOdJKc0cCRH06RE7hyUiUuCd5pd/PU+Pt16gPcnnY8vv5AnRncBYMnm/fxzRY4jGcHOy7uqdc+w0AmxtgQLc44xptq98XrPsJrRe4bFD71nWM1F4p5h+4/4q11zmiS77blnmDHmEPBllLMopVSt6XW0Sil1EqoTI8OUUqq6bGzRaqFVSsUVGy/v0kKrlIor2qJVSqko00KrlFJRpl0HSikVZTa2aPXyLqVUXInkCFwRGSwiX4nIJhGZfKKZtNAqpeJLhCqtiLiBvwGXAJ2AMSLS6UQiadeBUiquuCLXd9AD2GSM2QIgInOBy4D1Nd1R1AttPU/keqZFJCs0BaN1IpWtXkpk/0ls/cxOhlzNLP23/ODXPSMRp4xt/5Y1qTkikgWUv9HfzHJ/l9MITqBVKgf48YlkqmtdB87f+bBytmbTXDVjay6wN5utucIyxsw0xnQvt0TlB0ZdK7RKKRUrO4D0cuutQttqTAutUkpV7DOgg4i0FZFE4CrgjRPZUV07GWZNP1AFbM2muWrG1lxgbzZbc9WKMaZERCYB/wHcwLPGmHUnsq9qTfytlFLqxGnXgVJKRZkWWqWUirI6U2gjNRQu0kTkWRHJFZG1TmcpJSLpIrJIRNaLyDoRudnpTKVEpJ6IrBCRL0PZpjqdqTwRcYvIFyLyltNZSonIVhFZIyKrRCTb6TylRKSRiLwiIhtFZIOIRPYC3ThSJ/poQ0Ph/gcMJHjR8GfAGGNMjUdoRJqIXAgcBv5hjOnidB4AEWkJtDTGfC4iDYCVwHBLPi8Bko0xh0UkAVgK3GyM+dThaACIyG1Ad6ChMWaY03kgWGiB7saYvU5nKU9E5gBLjDHPhM7KJxljDjidy0Z1pUVbNhTOGFMElA6Fc5wx5iNgv9M5yjPG7DLGfB56nA9sIDjKxXEm6HBoNSG0WPHTXkRaAUOBZ5zOYjsRSQUuBGYDGGOKtMhWrq4U2oqGwllROGwnIhnAucByZ5N8J/Tr+SogF3jfGGNLtr8AdwABp4N8jwHeE5GVoSGjNmgL7AGeC3W1PCMiyU6HslVdKbTqBIhICvAqcEvolvFWMMb4jTHnEBxp00NEHO9yEZFhQK4xZqXTWSpwgTGmG8FZpG4MdVc5zQN0A54yxpwLHAGsOXdim7pSaCM2FO5kEer/fBV4wRjzmtN5KhL6VXMRMNjpLEAv4NJQf+hcoL+I/MvZSEHGmB2hP3OB+QS70pyWA+SU+23kFYKFV1WgrhTaiA2FOxmETjjNBjYYYx5xOk95ItJcRBqFHtcneIJzo7OpwBgzxRjTyhiTQfD79YEx5mcOx0JEkkMnNAn9aj4IcPwKF2PMbmC7iJwR2jSAE5g+8GRRJ4bgRnIoXKSJyEtAX6CZiOQA9xpjZjubil7AtcCaUF8owF3GmLcdzFSqJTAndCWJC5hnjLHmUioLeYH5wZ+deIAXjTHvOhupzE3AC6HGzxbg5w7nsVaduLxLKaXqsrrSdaCUUnWWFlqllIoyLbRKKRVlWmiVUirKtNAqpVSUaaFVSqko00KrlFJR9v82k2AV4ZGZfAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing remarks\n"
      ],
      "metadata": {
        "id": "YXP2KaVXzye6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_NPXLvJeSq8"
      },
      "source": [
        "## Some tips!\n",
        "\n",
        "* save your experiments - training takes time, and sometimes you want to check something later or reuse your model. As such, you should save your model, hyperparameters, data, training curves, etc., and keep it.\n",
        "\n",
        "**Example:** you trained a few models, evaluated their accuracy, and you got rid of them. Later, you were asked to assess the rocAUC too. If you discarded your model, you would have to retrain, which can take a few days.\n",
        "\n",
        "* the devil is in the details - many things in deep learning depend on little details. Errors are often silent. Due to this, you should adopt a defensive posture and evaluate and debug intermediate results as much as possible. In the end, you will save time.\n",
        "\n",
        "* do not reinvent the wheel unless you are having fun - many of the things you need are either implemented on PyTorch or other repositories. If you are implementing something by yourself, make sure you are wasting time because you want to."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Tutorial\n",
        "\n",
        "04/03 - 21:00\n",
        "\n",
        "**Topics:** GANs and VAEs and Metric Learning\n",
        "\n",
        "Pedro Neto\n",
        "\n",
        "Eduardo Castro\n",
        "\n",
        "<center><h1>Thank you!!</h1></center>\n"
      ],
      "metadata": {
        "id": "r2OEodiMz5De"
      }
    }
  ]
}